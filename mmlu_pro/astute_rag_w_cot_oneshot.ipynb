{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyen1122/solar_rag/blob/main/mmlu_pro/astute_rag_w_cot_oneshot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nyH8sMMs2RD-",
        "outputId": "6abe96ee-6c61-4f50-f713-b323c23d6cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-upstage\n",
            "  Downloading langchain_upstage-0.7.5-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting langchain-core<2.0.0,>=1.0.3 (from langchain-upstage)\n",
            "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain-openai<2.0.0,>=1.0.2 (from langchain-upstage)\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pypdf<5.0.0,>=4.2.0 (from langchain-upstage)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from langchain-upstage) (2.32.4)\n",
            "Collecting tokenizers<0.21.0,>=0.20.0 (from langchain-upstage)\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.31.0 (from langchain-upstage)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.43)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-upstage) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-upstage) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-upstage) (4.15.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (0.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers<0.21.0,>=0.20.0->langchain-upstage) (0.36.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.3->langchain-upstage) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (2024.11.6)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_upstage-0.7.5-py3-none-any.whl (20 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, typing-inspect, tokenizers, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-upstage, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.80\n",
            "    Uninstalling langchain-core-0.3.80:\n",
            "      Successfully uninstalled langchain-core-0.3.80\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "transformers 4.57.1 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.1.0 langchain-openai-1.0.3 langchain-text-splitters-1.0.0 langchain-upstage-0.7.5 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-4.3.1 requests-2.32.5 tokenizers-0.20.3 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-upstage langchain-community pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pdBgMTNwMuCm",
        "outputId": "bc3ffd55-7fd7-406f-bd5f-ad8690e99c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2025.11.12)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=a6d261ef0698f0062a277352f56a0aeacfa33ebbe4345cada7c315877a013c81\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3c/79/b36253689d838af4a0539782853ac3cc38a83a6591ad570dde\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.8.1\n"
          ]
        }
      ],
      "source": [
        "pip install wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C-AXD7fLRJI-",
        "outputId": "80a7622d-54a2-4ffe-fc46-a13c43617ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8qWw7Tq17dHw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "\n",
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "import wikipediaapi\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u3yQM7bzVIz",
        "outputId": "6cf48db8-6cee-42f6-b367-f93ee3903079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gavGzRrztXz",
        "outputId": "7e744abf-810f-412b-fc57-87fcb3a2065b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1gjkhHrJH8XBIbzomsGjznXFCi8ehroZK/nlp team project\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/nlp team project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c-yFVp9vQ26G"
      },
      "outputs": [],
      "source": [
        "UPSTAGE_API_KEY = \"up_VYzFNHEoEJPfAwYUNp5v9n1CPnMOm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cgvozRFhCJMz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"testset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iK1l_XYgUPay"
      },
      "outputs": [],
      "source": [
        "llm_classifier = ChatUpstage(\n",
        "    api_key=UPSTAGE_API_KEY,\n",
        "    model=\"solar-pro2\",\n",
        "    temperature=0.1, #거의 항상 같은 결과, 안정적이게\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oElUzmTxV6M-"
      },
      "outputs": [],
      "source": [
        "llm_solver = ChatUpstage(\n",
        "    api_key=UPSTAGE_API_KEY,\n",
        "    model=\"solar-pro2\",\n",
        "    temperature=0.2, #좀 더 유연한 답변을 내도록\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T6fjf7tL0E6A"
      },
      "outputs": [],
      "source": [
        "category_index = {\n",
        "    \"law\":        \"/content/drive/MyDrive/nlp team project/code/mmlu_category/law\",\n",
        "    \"psychology\": \"/content/drive/MyDrive/nlp team project/code/mmlu_category/psychology\",\n",
        "    \"business\":   \"/content/drive/MyDrive/nlp team project/code/mmlu_category/business\",\n",
        "    \"philosophy\": \"/content/drive/MyDrive/nlp team project/code/mmlu_category/philosophy\",\n",
        "    \"history\":    \"/content/drive/MyDrive/nlp team project/code/mmlu_category/history\",\n",
        "}\n",
        "\n",
        "categories = list(category_index.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wvfgJRYeP9M4"
      },
      "outputs": [],
      "source": [
        "\n",
        "emb = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=\"solar-embedding-1-large-passage\")\n",
        "\n",
        "# 카테고리별 faiss 한번에 로드해서 캐시\n",
        "vectorstore = {}\n",
        "for cat, path in category_index.items():\n",
        "    vectorstore[cat] = FAISS.load_local(\n",
        "        folder_path=path,\n",
        "        embeddings=emb,\n",
        "        allow_dangerous_deserialization=True,\n",
        "    )\n",
        "\n",
        "wiki = wikipediaapi.Wikipedia(user_agent= \"NLP-RAG/1.0\", language = 'en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRn3avDKKexv"
      },
      "source": [
        "**1st LLM**:\n",
        "\n",
        "- return category\n",
        "\n",
        "- return 3 keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nzr6RkbISTas"
      },
      "outputs": [],
      "source": [
        "category_prompt_template = \"\"\"\n",
        "You are an expert exam classifier for MMLU-Pro\n",
        "\n",
        "There are 5 possible categories:\n",
        "- law\n",
        "- psychology\n",
        "- business\n",
        "- philosophy\n",
        "- history\n",
        "\n",
        "[Task]\n",
        "Given a multiple-choice exam question (including all options),\n",
        "1. Choose one best category from the list above.\n",
        "2. Extract exactly 3 important keywords (one noun that consists of single word or short phrase)\n",
        "   that will be useful to search textbooks and Wikipedia.\n",
        "   Keywords should be as specific and accurate as possible (e.g., \"consent\",\n",
        "   \"cognitive dissonance\", \"Keynesian economics\").\n",
        "\n",
        "[Output format]\n",
        "Return a valid JSON with the following fields:\n",
        "- \"category\": one of [\"law\",\"psychology\",\"business\",\"philosophy\",\"history\"]\n",
        "- \"keywords\": a list of exactly 3 strings\n",
        "\n",
        "This is an output example:\n",
        "{{\n",
        "  \"category\": \"psychology\",\n",
        "  \"keywords\": [\"informed consent\", \"assent\", \"child counseling\"]\n",
        "}}\n",
        "\n",
        "[Question]\n",
        "{question}\n",
        "\"\"\".strip()\n",
        "\n",
        "category_prompt = ChatPromptTemplate.from_template(category_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "93dJK7O09Jar"
      },
      "outputs": [],
      "source": [
        "def classify_and_extract_keywords(question_text: str):\n",
        "    messages = category_prompt.format_messages(question=question_text)\n",
        "    resp = llm_classifier.invoke(messages)\n",
        "    raw = resp.content.strip()\n",
        "\n",
        "    if not raw:\n",
        "        # 완전 빈 응답이면 기본값으로 대충 처리 (죽지 않게)\n",
        "        print(\"[WARN] Empty LLM output for category, fallback to 'history'\")\n",
        "        return \"history\", [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "    # 1차 시도: 전체를 JSON으로 해석\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        # 2차 시도: 문자열 안에서 {...} 구간만 잘라서 해석\n",
        "        start = raw.find(\"{\")\n",
        "        end = raw.rfind(\"}\")\n",
        "        if start == -1 or end == -1:\n",
        "            print(\"[WARN] No JSON object found in output, fallback to 'history'\")\n",
        "            return \"history\", [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "        json_str = raw[start:end+1]\n",
        "        try:\n",
        "            data = json.loads(json_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"[WARN] JSON parse failed again:\", e)\n",
        "            return \"history\", [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "    category = data.get(\"category\", \"history\").strip().lower()\n",
        "    keywords = data.get(\"keywords\", [])\n",
        "    keywords = [str(k).strip() for k in keywords][:3]\n",
        "\n",
        "    if not keywords:\n",
        "        keywords = [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "    return category, keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e_n3aEbhUxxX"
      },
      "outputs": [],
      "source": [
        "def fetch_wiki_context(keywords, max_chars_per_page=1200):\n",
        "    snippets = []\n",
        "    for kw in keywords:\n",
        "        try:\n",
        "            page = wiki.page(kw)\n",
        "            if not page.exists():\n",
        "                print(f\"[WARN] page for '{kw}' does not exist\")\n",
        "                continue\n",
        "            text = page.text[:max_chars_per_page]\n",
        "            snippets.append(f\"[Wikipedia: {page.title}]\\n{text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] wikipediaapi fetch failed for '{kw}': {e}\")\n",
        "            continue\n",
        "\n",
        "    return \"\\n\\n\".join(snippets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TIwiC6e0e44n"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "############################################\n",
        "# 0. 상수 & 기본 설정\n",
        "############################################\n",
        "\n",
        "TOP_K_TEXTBOOK = 5\n",
        "TOP_K_WIKI = 3\n",
        "QUESTION_COL = \"prompts\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "# 1. 질문 + 옵션(A~I) 파싱\n",
        "############################################\n",
        "\n",
        "# 1. 질문 + 옵션(A~J) 파싱\n",
        "def parse_question_and_options(prompt: str):\n",
        "    \"\"\"\n",
        "    prompt: QUESTION + (A) ~ (J) 옵션이 한 문자열에 들어있는 형태\n",
        "    return: question_text(str), options(dict: 'A'~'J' -> str)\n",
        "    \"\"\"\n",
        "    pattern = r\"\\(([A-J])\\)\\s*\"\n",
        "    matches = list(re.finditer(pattern, prompt))\n",
        "\n",
        "    if not matches:\n",
        "        return prompt.strip(), {}\n",
        "\n",
        "    first = matches[0]\n",
        "    question_text = prompt[:first.start()].strip()\n",
        "\n",
        "    options: Dict[str, str] = {}\n",
        "    for idx, m in enumerate(matches):\n",
        "        letter = m.group(1)  # 'A'~'J'\n",
        "        start = m.end()\n",
        "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(prompt)\n",
        "        opt_text = prompt[start:end].strip()\n",
        "        options[letter] = opt_text\n",
        "\n",
        "    return question_text, options\n"
      ],
      "metadata": {
        "id": "lNFvwZRPpuBu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "# 2. \"Final Answer: X\" 파서\n",
        "############################################\n",
        "\n",
        "def extract_final_answer_letter(text: str) -> str:\n",
        "    \"\"\"\n",
        "    p_ans 출력에서 'Final Answer: X' 의 X(A~J)를 뽑기.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"Final Answer:\\s*([A-J])\\s*$\", text, flags=re.IGNORECASE | re.MULTILINE)\n",
        "    if m:\n",
        "        return m.group(1).upper()\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "zaonqmXWp2gf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################################\n",
        "# 3. Memory p_gen (INTERNAL 문서 생성)\n",
        "############################################\n",
        "\n",
        "P_MEM_TMPL = \"\"\"\n",
        "You are a knowledgeable expert in {category}.\n",
        "\n",
        "Task: Using ONLY your own internal knowledge (without seeing any external documents),\n",
        "generate a short document that provides accurate and relevant information to help\n",
        "answer the given exam question.\n",
        "\n",
        "If you truly do not know the answer or lack enough information, explicitly state\n",
        "\"I don't know\" rather than hallucinating facts.\n",
        "\n",
        "Exam Question:\n",
        "{question_text}\n",
        "\n",
        "Write ONE coherent document that:\n",
        "- Focuses only on concepts, definitions, and relations that are relevant to the question.\n",
        "- Is self-contained and clear enough to help another model answer the question.\n",
        "- Avoids unnecessary details.\n",
        "\n",
        "Document:\n",
        "\"\"\".strip()\n",
        "\n",
        "p_mem_prompt = ChatPromptTemplate.from_template(P_MEM_TMPL)\n",
        "\n",
        "\n",
        "def run_p_mem(category: str, question_text: str, llm=None) -> str:\n",
        "    if llm is None:\n",
        "        llm = llm_solver\n",
        "    msgs = p_mem_prompt.format_messages(\n",
        "        category=category,\n",
        "        question_text=question_text,\n",
        "    )\n",
        "    resp = llm.invoke(msgs)\n",
        "    return resp.content.strip()"
      ],
      "metadata": {
        "id": "bu5pWXR0p6mv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "# 4. Adaptive Passage Generation p_gen (각 external doc 하나씩)\n",
        "############################################\n",
        "\n",
        "P_GEN_TMPL = \"\"\"\n",
        "You are a careful domain expert in {category}.\n",
        "\n",
        "Your task is to generate a short document that provides accurate and relevant\n",
        "information to help answer the given exam question. You are given ONE passage\n",
        "retrieved from a {source_type} source (e.g., a textbook section or a Wikipedia paragraph).\n",
        "\n",
        "Use this passage as your primary evidence. If the information is unclear\n",
        "or truly missing, explicitly state \"I don't know\" rather than hallucinating\n",
        "new facts.\n",
        "\n",
        "Question:\n",
        "{question_text}\n",
        "\n",
        "Retrieved passage from {source_type} (Doc {doc_id}):\n",
        "{raw_passage}\n",
        "\n",
        "Now write ONE coherent document that:\n",
        "- Focuses only on information relevant to the question.\n",
        "- Paraphrases and condenses the useful parts of this passage.\n",
        "- Explains key concepts and relations that may help answer the question.\n",
        "- Omits irrelevant details.\n",
        "\n",
        "Document:\n",
        "\"\"\".strip()\n",
        "\n",
        "p_gen_prompt = ChatPromptTemplate.from_template(P_GEN_TMPL)\n",
        "\n",
        "\n",
        "def run_p_gen_for_docs(\n",
        "    category: str,\n",
        "    source_type: str,          # \"TEXTBOOK\" or \"WIKIPEDIA\"\n",
        "    question_text: str,\n",
        "    raw_passages: List[str],   # 각 chunk 텍스트\n",
        "    start_doc_idx: int = 1,\n",
        "    llm=None,\n",
        ") -> Tuple[str, int]:\n",
        "    \"\"\"\n",
        "    각 raw_passage마다 p_gen을 한 번씩 호출하고,\n",
        "    Astute 스타일의 [Doc N | SOURCE=...] 블록으로 이어 붙여서 리턴.\n",
        "    \"\"\"\n",
        "    if llm is None:\n",
        "        llm = llm_solver\n",
        "\n",
        "    doc_blocks = []\n",
        "    cur_idx = start_doc_idx\n",
        "\n",
        "    for k, passage in enumerate(raw_passages, start=1):\n",
        "        if not passage.strip():\n",
        "            continue\n",
        "\n",
        "        msgs = p_gen_prompt.format_messages(\n",
        "            category=category,\n",
        "            source_type=source_type,\n",
        "            question_text=question_text,\n",
        "            doc_id=k,\n",
        "            raw_passage=passage,\n",
        "        )\n",
        "        resp = llm.invoke(msgs)\n",
        "        doc_text = resp.content.strip()\n",
        "\n",
        "        block = (\n",
        "            f\"[Doc {cur_idx} | SOURCE=EXTERNAL({source_type}) | ORIG_ID={source_type}_{k}]\\n\"\n",
        "            f\"{doc_text}\"\n",
        "        )\n",
        "        doc_blocks.append(block)\n",
        "        cur_idx += 1\n",
        "\n",
        "    return \"\\n\\n\".join(doc_blocks), cur_idx"
      ],
      "metadata": {
        "id": "L_pJ7Jj7p_Bs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################################\n",
        "# 5. Retrieval helpers (textbook / wiki)\n",
        "############################################\n",
        "\n",
        "def get_textbook_passages(category: str, query: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    category별 vectorstore에서 TOP_K_TEXTBOOK개 passage 가져와 리스트로 반환.\n",
        "    \"\"\"\n",
        "    vs = vectorstore[category]\n",
        "    docs = vs.similarity_search(query, k=TOP_K_TEXTBOOK)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "\n",
        "def get_wiki_passages(keywords: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    간단히 fetch_wiki_context로 받은 문자열을 하나의 passage로 처리.\n",
        "    필요하면 여기서 여러 문단으로 split하여 리스트로 만들 수 있음.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        raw = fetch_wiki_context(keywords)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Wikipedia retrieval failed: {e}\")\n",
        "        return []\n",
        "\n",
        "    raw = (raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return []\n",
        "    return [raw]  # 일단 1개 passage로 사용\n"
      ],
      "metadata": {
        "id": "bQD40gJZ6969"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "# 6. Iterative Knowledge Consolidation p_con\n",
        "############################################\n",
        "\n",
        "P_CON_TMPL = \"\"\"\n",
        "Task: Consolidate information from both your own memorized documents (INTERNAL)\n",
        "and externally retrieved documents (EXTERNAL) in response to the given exam question.\n",
        "\n",
        "Guidelines:\n",
        "* For documents that provide consistent information, cluster them together and\n",
        "  summarize the key details into a single, concise document.\n",
        "* For documents with conflicting information, separate them into distinct documents,\n",
        "  ensuring each document captures the unique perspective or data.\n",
        "* Exclude any information that is irrelevant to the question.\n",
        "* For each NEW document you create, clearly indicate:\n",
        "  - Whether the source was INTERNAL(MEMORY) or EXTERNAL(TEXTBOOK/WIKIPEDIA).\n",
        "  - The original document numbers that contributed to it (e.g., FROM=1,3,4).\n",
        "\n",
        "Initial Context (numbered documents):\n",
        "{context_init}\n",
        "\n",
        "Last Context (may be empty):\n",
        "{last_context}\n",
        "\n",
        "Question:\n",
        "{question_text}\n",
        "\n",
        "Now produce your New Context as a list of numbered documents in the following style:\n",
        "\n",
        "[Doc 1 | SOURCE=EXTERNAL(TEXTBOOK) | FROM=2,4]\n",
        "...document text...\n",
        "\n",
        "[Doc 2 | SOURCE=INTERNAL(MEMORY) | FROM=1]\n",
        "...document text...\n",
        "\n",
        "New Context:\n",
        "\"\"\".strip()\n",
        "\n",
        "p_con_prompt = ChatPromptTemplate.from_template(P_CON_TMPL)\n",
        "\n",
        "\n",
        "def run_p_con(\n",
        "    question_text: str,\n",
        "    context_init: str,\n",
        "    last_context: str = \"\",\n",
        "    llm=None,\n",
        ") -> str:\n",
        "    if llm is None:\n",
        "        try:\n",
        "            llm = llm_solar_pro\n",
        "        except NameError:\n",
        "            llm = llm_solver\n",
        "\n",
        "    msgs = p_con_prompt.format_messages(\n",
        "        question_text=question_text,\n",
        "        context_init=context_init,\n",
        "        last_context=last_context,\n",
        "    )\n",
        "    resp = llm.invoke(msgs)\n",
        "    return resp.content.strip()\n"
      ],
      "metadata": {
        "id": "_Zpu4HRa6-rt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "# 7. Knowledge Consolidation + Answer Finalization p_ans\n",
        "############################################\n",
        "\n",
        "P_ANS_TMPL = \"\"\"\n",
        "You are an expert exam solver for MMLU-Pro.\n",
        "\n",
        "Task: Answer the given multiple-choice question using the consolidated information\n",
        "from both your own memorized knowledge (INTERNAL) and the externally retrieved documents.\n",
        "\n",
        "Below is an example of how you should reason and format your answer.\n",
        "\n",
        "[EXAMPLE]\n",
        "\n",
        "Initial Context:\n",
        "(INTERNAL) In classical conditioning, a neutral stimulus is paired with an unconditioned stimulus.\n",
        "\n",
        "[Consolidated Context]:\n",
        "(EXTERNAL-TEXTBOOK) Classical conditioning was first systematically described by Ivan Pavlov.\n",
        "(EXTERNAL-WIKIPEDIA) Classical conditioning associates a neutral stimulus with a reflex response.\n",
        "\n",
        "[QUESTION]\n",
        "In classical conditioning, the neutral stimulus should:\n",
        "(A) Immediately elicit a response before any pairing.\n",
        "(B) Be repeatedly paired with the unconditioned stimulus.\n",
        "(C) Be presented only after extinction has occurred.\n",
        "(D) Remain completely unrelated to the unconditioned stimulus.\n",
        "\n",
        "Reasoning:\n",
        "Step 1 – Consolidate information again (mentally)\n",
        "- INTERNAL says a neutral stimulus is paired with an unconditioned stimulus.\n",
        "- TEXTBOOK says Pavlov described this process of pairing.\n",
        "- WIKIPEDIA says the neutral stimulus becomes associated with a reflex response.\n",
        "- All sources agree that repeated pairing is essential.\n",
        "\n",
        "Step 2 – Extract essential clues from the QUESTION\n",
        "- Clue 1: The question asks about the role of the neutral stimulus in classical conditioning.\n",
        "- Clue 2: The neutral stimulus does NOT initially elicit the response.\n",
        "- Clue 3: The neutral stimulus must be paired with the unconditioned stimulus.\n",
        "\n",
        "Step 3 – Compare each option (A–D)\n",
        "- (A) is contradicted: the neutral stimulus does not elicit a response before pairing.\n",
        "- (B) matches the clues: it exactly describes repeated pairing with the unconditioned stimulus.\n",
        "- (C) is irrelevant: extinction is not mentioned in the clues or context.\n",
        "- (D) contradicts the idea of association and pairing.\n",
        "\n",
        "Step 4 – Select the Final Answer\n",
        "Option (B) is fully consistent with all clues and evidence,\n",
        "whereas the other options are contradicted or unsupported.\n",
        "\n",
        "Final Answer: B\n",
        "\n",
        "Now follow the same procedure for the NEW problem below.\n",
        "\n",
        "Step 1 – Consolidate information again (mentally)\n",
        "* Read the Initial Context and the Consolidated Context.\n",
        "* Identify the key facts and note whether each comes mainly from INTERNAL(MEMORY)\n",
        "  or EXTERNAL(TEXTBOOK/WIKIPEDIA).\n",
        "* Group them into a small set of evidence clusters.\n",
        "* Exclude any information that is irrelevant to the question.\n",
        "* If INTERNAL and EXTERNAL disagree, explain which seems more reliable and why.\n",
        "* If two EXTERNAL sources disagree, treat TEXTBOOK as more reliable than WIKIPEDIA.\n",
        "\n",
        "Step 2 – Extract essential clues from the QUESTION\n",
        "* Look only at the QUESTION (stem + options) and extract 3–5 essential clues\n",
        "  (core concepts, constraints, or relationships).\n",
        "* If these QUESTION clues conflict with any context, ALWAYS trust the QUESTION clues.\n",
        "\n",
        "Step 3 – Compare each option (A–J)\n",
        "For each option:\n",
        "* Explain briefly (1–3 sentences) whether it is supported or contradicted by the\n",
        "  evidence clusters and QUESTION clues.\n",
        "* If there is not enough information, say so but still compare it to other options.\n",
        "\n",
        "Step 4 – Select the Final Answer\n",
        "* Choose the SINGLE best option (A–J) that fits the QUESTION clues and the\n",
        "  strongest evidence.\n",
        "* If the context is incomplete, still choose the MOST reasonable option that does\n",
        "  not contradict the evidence.\n",
        "* Provide a short explanation (3–6 sentences) summarizing why this option is better\n",
        "  than the others.\n",
        "\n",
        "Important formatting rules:\n",
        "- At the VERY END, output the final answer in the exact format:\n",
        "  Final Answer: X\n",
        "- Replace X with a single capital letter between A and J.\n",
        "- Do NOT wrap the line in Markdown (no **, no backticks).\n",
        "- Do NOT put the letter in parentheses or quotes.\n",
        "- Do NOT add any text after the 'Final Answer: X' line.\n",
        "- This line MUST appear only once.\n",
        "\n",
        "Initial Context:\n",
        "{context_init}\n",
        "\n",
        "[Consolidated Context]:\n",
        "{context_con}\n",
        "\n",
        "[QUESTION]\n",
        "{question_with_options}\n",
        "\n",
        "Now reason step by step according to the steps above, then give the final answer.\n",
        "\"\"\".strip()\n",
        "\n",
        "p_ans_prompt = ChatPromptTemplate.from_template(P_ANS_TMPL)\n",
        "\n",
        "\n",
        "def run_p_ans(\n",
        "    context_init: str,\n",
        "    context_con: str,\n",
        "    full_question: str,\n",
        "    llm=None,\n",
        ") -> Dict[str, Any]:\n",
        "    if llm is None:\n",
        "        try:\n",
        "            llm = llm_solar_pro\n",
        "        except NameError:\n",
        "            llm = llm_solver\n",
        "\n",
        "    msgs = p_ans_prompt.format_messages(\n",
        "        context_init=context_init,\n",
        "        context_con=context_con,\n",
        "        question_with_options=full_question,\n",
        "    )\n",
        "    resp = llm.invoke(msgs)\n",
        "    raw = resp.content.strip()\n",
        "    letter = extract_final_answer_letter(raw)\n",
        "    return {\"final_answer\": letter, \"raw_reasoning\": raw}"
      ],
      "metadata": {
        "id": "aEK5ci917MW5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "# 8. 한 문제 풀기 (Astute-RAG 스타일)\n",
        "############################################\n",
        "\n",
        "def solve_mmlu_astute_style(\n",
        "    full_prompt: str,\n",
        "    use_wiki: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    1) question/option 파싱\n",
        "    2) category, keywords 분류\n",
        "    3) memory doc + textbook/wiki passages -> p_gen\n",
        "    4) p_con으로 consolidated context 생성\n",
        "    5) p_ans로 최종 답 선택\n",
        "    \"\"\"\n",
        "    # 1) 질문/선지 분리\n",
        "    question_text, options = parse_question_and_options(full_prompt)\n",
        "\n",
        "    # 2) category / keywords\n",
        "    category, keywords = classify_and_extract_keywords(question_text)\n",
        "    if category not in vectorstore:\n",
        "        print(f\"[WARN] Unknown category '{category}', fallback to 'business'\")\n",
        "        category = \"business\"\n",
        "\n",
        "    # 3) Retrieval\n",
        "    retrieval_query = question_text + \" \" + \" \".join(options.values())\n",
        "    tb_passages = get_textbook_passages(category, retrieval_query)\n",
        "    wiki_passages = get_wiki_passages(keywords) if use_wiki else []\n",
        "\n",
        "    # 4) p_mem: INTERNAL 문서\n",
        "    doc_blocks = []\n",
        "    next_doc_idx = 1\n",
        "\n",
        "    mem_doc_text = run_p_mem(category, question_text)\n",
        "    if mem_doc_text.strip():\n",
        "        doc_blocks.append(\n",
        "            f\"[Doc {next_doc_idx} | SOURCE=INTERNAL(MEMORY)]\\n{mem_doc_text}\"\n",
        "        )\n",
        "        next_doc_idx += 1\n",
        "\n",
        "    # 5) p_gen: TEXTBOOK docs\n",
        "    tb_block, next_doc_idx = run_p_gen_for_docs(\n",
        "        category=category,\n",
        "        source_type=\"TEXTBOOK\",\n",
        "        question_text=question_text,\n",
        "        raw_passages=tb_passages,\n",
        "        start_doc_idx=next_doc_idx,\n",
        "        llm=llm_solver,\n",
        "    )\n",
        "    if tb_block.strip():\n",
        "        doc_blocks.append(tb_block)\n",
        "\n",
        "    # 6) p_gen: WIKIPEDIA docs\n",
        "    if use_wiki and wiki_passages:\n",
        "        wiki_block, next_doc_idx = run_p_gen_for_docs(\n",
        "            category=category,\n",
        "            source_type=\"WIKIPEDIA\",\n",
        "            question_text=question_text,\n",
        "            raw_passages=wiki_passages,\n",
        "            start_doc_idx=next_doc_idx,\n",
        "            llm=llm_solver,\n",
        "        )\n",
        "        if wiki_block.strip():\n",
        "            doc_blocks.append(wiki_block)\n",
        "\n",
        "    context_init = \"\\n\\n\".join(doc_blocks)\n",
        "\n",
        "    # 7) p_con: consolidation (한 번만)\n",
        "    context_con = run_p_con(\n",
        "        question_text=question_text,\n",
        "        context_init=context_init,\n",
        "        last_context=\"\",\n",
        "    )\n",
        "\n",
        "    # 8) p_ans: 최종 정답\n",
        "    ans = run_p_ans(\n",
        "        context_init=context_init,\n",
        "        context_con=context_con,\n",
        "        full_question=full_prompt,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"final_answer\": ans[\"final_answer\"],\n",
        "        \"category\": category,\n",
        "        \"keywords\": keywords,\n",
        "        \"context_init\": context_init,\n",
        "        \"context_con\": context_con,\n",
        "        \"raw_reasoning\": ans[\"raw_reasoning\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "KMrStAyL7gBQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "50개"
      ],
      "metadata": {
        "id": "UnvvY2pF5bcc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT2RVjaYHJtP",
        "outputId": "9903b043-b747-49d1-c7f2-16e6f10d7bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[83] Solving: QUESTION240) Daniel receives at 6.5% commission on all sales...\n",
            "[WARN] page for 'percentage calculation' does not exist\n",
            "[53] Solving: QUESTION10878) Which branch of Judaism founded by Zacharias ...\n",
            "[WARN] page for 'Positive-Historical Judaism' does not exist\n",
            "[70] Solving: QUESTION2694) Which of the following is not an available too...\n",
            "[WARN] page for 'suicide risk' does not exist\n",
            "[WARN] page for 'assessment tools' does not exist\n",
            "[45] Solving: QUESTION1485) Hume's attack on natural law is founded on his...\n",
            "[44] Solving: QUESTION10881) \"The minor premise must affirm the antecedent...\n",
            "[39] Solving: QUESTION11239) Zhuangzi describes a state as ziran, which me...\n",
            "[22] Solving: QUESTION11192) Select the best translation into predicate lo...\n",
            "[80] Solving: QUESTION4715) This question refers to the following informat...\n",
            "[WARN] page for 'Colonel Cresap' does not exist\n",
            "[10] Solving: QUESTION2449) Dr. Ryan is a psychotherapist in a small town....\n",
            "[WARN] page for 'therapist-client boundary' does not exist\n",
            "[0] Solving: QUESTION4868) In 1797, John Frere made a discovery that he d...\n",
            "[18] Solving: QUESTION2054) What are the assumptions concerning an individ...\n",
            "[30] Solving: QUESTION4886) This question refers to the following informat...\n",
            "[WARN] page for 'Logan's Address' does not exist\n",
            "[WARN] page for 'Colonel Cresap' does not exist\n",
            "[WARN] page for 'Indian policies' does not exist\n",
            "[73] Solving: QUESTION290) On October 17, Thomas Long purchased two $1,000...\n",
            "[WARN] page for 'brokerage fee' does not exist\n",
            "[WARN] page for 'net proceeds' does not exist\n",
            "[33] Solving: QUESTION1601) A federal statute provides states with funds f...\n",
            "[WARN] page for 'federal standards' does not exist\n",
            "[90] Solving: QUESTION936) A defendant, an indigent, was arrested and char...\n",
            "[WARN] page for 'mental incompetency' does not exist\n",
            "[4] Solving: QUESTION1326) A large privately owned and operated shopping ...\n",
            "[76] Solving: QUESTION4741) This question refers to the following informat...\n",
            "[77] Solving: QUESTION425) What is the date of maturity of a 60-day note d...\n",
            "[WARN] page for 'note maturity' does not exist\n",
            "[WARN] page for '60-day note' does not exist\n",
            "[12] Solving: QUESTION10850) One objection to Singer’s theory that he cons...\n",
            "[WARN] page for 'Singer’s theory' does not exist\n",
            "[WARN] page for 'ethical obligations' does not exist\n",
            "[31] Solving: QUESTION644) An invoice of $10,000 is marked (6/10), (n/30)....\n",
            "[55] Solving: QUESTION4938) This question refers to the following informat...\n",
            "[88] Solving: QUESTION4724) This question refers to the following informat...\n",
            "[26] Solving: QUESTION4942) This question refers to the following informat...\n",
            "[42] Solving: QUESTION5019) This question refers to the following informat...\n",
            "[69] Solving: QUESTION2643) Research into ___________ has helped us unders...\n",
            "[WARN] page for 'paradoxical reward' does not exist\n",
            "[15] Solving: QUESTION2368) What are the two principal explanations for th...\n",
            "[WARN] page for 'limited capacity' does not exist\n",
            "[40] Solving: QUESTION2386) Which of the following strategies would probab...\n",
            "[WARN] page for 'behavioral strategies' does not exist\n",
            "[96] Solving: QUESTION208) You are asked to determine the price of a Europ...\n",
            "[9] Solving: QUESTION10952) Construct a complete truth table for the foll...\n",
            "[72] Solving: QUESTION967) A city imposes a municipal excise tax of $200 p...\n",
            "[WARN] page for 'itinerant artists' does not exist\n",
            "[11] Solving: QUESTION1979) A landscaper agreed to maintain the yard of a ...\n",
            "[47] Solving: QUESTION2519) Babbling ordinarily begins at about 4 to 5 mon...\n",
            "[WARN] page for 'infant speech' does not exist\n",
            "[85] Solving: QUESTION1286) Which of the following criticisms of Llewellyn...\n",
            "[WARN] page for 'formal style' does not exist\n",
            "[28] Solving: QUESTION287) An automobile that cost $3,000 four years ago i...\n",
            "[WARN] page for 'automobile value' does not exist\n",
            "[WARN] page for 'average yearly loss' does not exist\n",
            "[93] Solving: QUESTION11274) The universe, like a watch, must have a maker...\n",
            "[5] Solving: QUESTION1769) A farmer owned a 40-acre tract of farmland loc...\n",
            "[WARN] page for 'option to purchase' does not exist\n",
            "[66] Solving: QUESTION11137) Naturalists who concentrated on natural eleme...\n",
            "[65] Solving: QUESTION2526) Jupiter pilots his newly created perfectionism...\n",
            "[WARN] page for 'perfectionism scale' does not exist\n",
            "[WARN] page for 'correlates' does not exist\n",
            "[35] Solving: QUESTION10914) Carens's main conclusion is that (A) liberal ...\n",
            "[WARN] page for 'moral requirement' does not exist\n",
            "[16] Solving: QUESTION4834) This question refers to the following informat...\n",
            "[WARN] page for 'Christian Nobility' does not exist\n",
            "[49] Solving: QUESTION962) A plaintiff sued a defendant for injuries that ...\n",
            "[WARN] page for 'crosswalk statute' does not exist\n",
            "[WARN] page for 'neighbor testimony' does not exist\n",
            "[34] Solving: QUESTION11104) All things that are spoiled are inedible. Tim...\n",
            "[7] Solving: QUESTION1163) A mechanic agreed in writing to make repairs t...\n",
            "[WARN] page for 'contract modification' does not exist\n",
            "[95] Solving: QUESTION307) The enforcement of company privacy is complex a...\n",
            "[WARN] page for 'corporate privacy' does not exist\n",
            "[27] Solving: QUESTION1114) A defendant hated a victim and decided to kill...\n",
            "[19] Solving: QUESTION373) Mr. Joseph Miles and Mr. Gary Rose are partners...\n",
            "[WARN] page for 'salary allowance' does not exist\n",
            "[81] Solving: QUESTION11042) Richardson-Self argues that sexist speech (A)...\n",
            "[WARN] page for 'sexist speech' does not exist\n",
            "[WARN] page for 'Richardson-Self' does not exist\n",
            "[WARN] page for 'linguistic ethics' does not exist\n",
            "[25] Solving: QUESTION2631) Group A consists of people whose measured inte...\n",
            "[WARN] page for 'interest similarity' does not exist\n",
            "[WARN] page for 'career alignment' does not exist\n",
            "[WARN] page for 'engineering interests' does not exist\n",
            "[62] Solving: QUESTION769) Calculate the Gross Domestic Product using the ...\n",
            "[WARN] page for 'total expenditure approach' does not exist\n",
            "[13] Solving: QUESTION1422) A company offered to sell several loads of lan...\n",
            "저장 완료: astute-rag-1121-50sample.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/testset_100 (1).csv\")\n",
        "\n",
        "# 🔹 100개 중 랜덤 50개 인덱스만 선택\n",
        "sample_indices = df.sample(n=50, random_state=42).index   # random_state 바꾸면 샘플 바뀜\n",
        "\n",
        "for i in sample_indices:\n",
        "    row = df.loc[i]\n",
        "    full_prompt = row[QUESTION_COL]\n",
        "\n",
        "    print(f\"[{i}] Solving: {str(full_prompt)[:60]}...\")\n",
        "\n",
        "    out = solve_mmlu_astute_style(\n",
        "        full_prompt=full_prompt,\n",
        "        use_wiki=True,\n",
        "    )\n",
        "\n",
        "    final_ans = out[\"final_answer\"]\n",
        "    df.loc[i, \"astute_answer\"] = final_ans\n",
        "    df.loc[i, \"pred_category\"] = out[\"category\"]\n",
        "\n",
        "    kws = out[\"keywords\"]\n",
        "    df.loc[i, \"kw1\"] = kws[0] if len(kws) > 0 else \"\"\n",
        "    df.loc[i, \"kw2\"] = kws[1] if len(kws) > 1 else \"\"\n",
        "    df.loc[i, \"kw3\"] = kws[2] if len(kws) > 2 else \"\"\n",
        "\n",
        "    # reasoning / context 저장\n",
        "    df.loc[i, \"cot_full\"] = out[\"raw_reasoning\"]\n",
        "    df.loc[i, \"context_init\"] = out[\"context_init\"]\n",
        "    df.loc[i, \"context_con\"] = out[\"context_con\"]\n",
        "\n",
        "out_name = \"astute-rag-1121-50sample.csv\"\n",
        "df.to_csv(out_name, index=False)\n",
        "print(f\"저장 완료: {out_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZhemODlHQsM9"
      },
      "outputs": [],
      "source": [
        "df_sample = df.loc[sample_indices].copy()\n",
        "df_sample.to_csv(\"astute-rag-1121-50only.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존 testset.csv"
      ],
      "metadata": {
        "id": "V81fETNv5sqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"testset.csv\")[26:]  # 질문, 선택지 다 합쳐진 컬럼 가정\n",
        "QUESTION_COL = \"prompts\"\n",
        "\n",
        "results = []\n",
        "for i, row in df.iterrows():\n",
        "    q = row[QUESTION_COL]\n",
        "    print(f\"[{i}] Solving: {q[:60]}...\")\n",
        "    out = solve_mmlu_astute_style(q)\n",
        "    results.append(out[\"final_answer\"])\n",
        "    df.loc[i, \"pred_category\"] = out[\"category\"]\n",
        "    df.loc[i, \"kw1\"] = out[\"keywords\"][0] if len(out[\"keywords\"]) > 0 else \"\"\n",
        "    df.loc[i, \"kw2\"] = out[\"keywords\"][1] if len(out[\"keywords\"]) > 1 else \"\"\n",
        "    df.loc[i, \"kw3\"] = out[\"keywords\"][2] if len(out[\"keywords\"]) > 2 else \"\"\n",
        "    df.loc[i, \"rag_cot_answer\"] = out[\"final_answer\"]\n",
        "    df.loc[i, \"cot_full\"] = out[\"raw_reasoning\"]\n",
        "\n",
        "df.to_csv(\"baseline-1120.csv\", index=False)\n",
        "print(\"저장 완료:baseline-1120.csv\")"
      ],
      "metadata": {
        "id": "_uFFQoQp5u4s",
        "outputId": "1e95de4d-79fa-4812-8d17-d70acccfb14b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26] Solving: QUESTION27) A man is at home in his apartment, alone, late a...\n",
            "[27] Solving: QUESTION28) What do Homo sapiens and Australopithecus afaren...\n",
            "[28] Solving: QUESTION29)This question refers to the following information...\n",
            "[WARN] page for 'frontier garrisons' does not exist\n",
            "[29] Solving: QUESTION30)Homo erectus differed from Homo habilis in which ...\n",
            "[WARN] page for 'evolutionary differences' does not exist\n",
            "[30] Solving: QUESTION31)During the manic phase of a bipolar disorder, ind...\n",
            "[WARN] page for 'manic phase' does not exist\n",
            "[31] Solving: QUESTION32) This question refers to the following informatio...\n",
            "[WARN] page for 'Germany's responsibility' does not exist\n",
            "[32] Solving: QUESTION33) You receive a phone call from Hermann H., age 28...\n",
            "[WARN] page for 'ethical psychologist' does not exist\n",
            "[33] Solving: QUESTION34) During the second stage of Kohlberg’s preconvent...\n",
            "[WARN] page for 'preconventional level' does not exist\n",
            "[34] Solving: QUESTION35)  In satisfying Kant's Humanity formulation of th...\n",
            "[WARN] page for 'Kant's Humanity formulation' does not exist\n",
            "[WARN] page for 'morally permissible' does not exist\n",
            "[35] Solving: QUESTION36) Aristotle says  that what makes things be what t...\n",
            "[36] Solving: QUESTION37) The ________ School of jurisprudence believes th...\n",
            "[WARN] page for 'legal traditions' does not exist\n",
            "[37] Solving: QUESTION38) A woman was standing in the aisle of a subway ca...\n",
            "[WARN] page for 'criminal offenses' does not exist\n",
            "[38] Solving: QUESTION39) A defendant met her friend at the electronics st...\n",
            "[39] Solving: QUESTION40)____________ refers to a strategic process involv...\n",
            "[WARN] page for 'stakeholder assessment' does not exist\n",
            "[WARN] page for 'long-term relationships' does not exist\n",
            "[40] Solving: QUESTION41)This question refers to the following information...\n",
            "[WARN] page for 'ancient Mesopotamian literature' does not exist\n",
            "[41] Solving: QUESTION42) Is the recognition of foreign judgments subject ...\n",
            "[42] Solving: QUESTION43) Some contemporary intelligence researchers like ...\n",
            "[43] Solving: QUESTION44) BobGafneyand Susan Medina invested $40,000 and $...\n",
            "[WARN] page for 'interest on investment' does not exist\n",
            "[WARN] page for 'partnership division' does not exist\n",
            "[44] Solving: QUESTION45) One objection to Singer’s theory that he conside...\n",
            "[WARN] page for 'Singer’s theory' does not exist\n",
            "[45] Solving: QUESTION46) In 1797, John Frere made a discovery that he des...\n",
            "[46] Solving: QUESTION47) Pick the correct description of the following te...\n",
            "[47] Solving: QUESTION48) Which of the following describes a key change in...\n",
            "[48] Solving: QUESTION49) Delia was accepted to both Harvard University an...\n",
            "[49] Solving: QUESTION50) Which is the least accurate description of legal...\n",
            "저장 완료:baseline-1120.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. 파일 경로\n",
        "GT_PATH = \"testset.csv\"          # 정답 파일\n",
        "PRED_PATH = \"baseline-1120.csv\"       # 출력 파일\n",
        "\n",
        "# 2. 컬럼 이름\n",
        "GT_COL = \"answers\"               # testset.csv에서 정답 컬럼\n",
        "PRED_COL = \"rag_cot_answer\"      # baseline.csv에서 예측 컬럼\n",
        "\n",
        "# 3. csv 로드\n",
        "gt_df = pd.read_csv(GT_PATH)[26:]      # 26번 문제부터라고 가정\n",
        "pred_df = pd.read_csv(PRED_PATH)\n",
        "\n",
        "# 4. Series 추출\n",
        "gt = gt_df[GT_COL].astype(str)\n",
        "pred = pred_df[PRED_COL].astype(str)\n",
        "\n",
        "# 5. 정규화 함수\n",
        "def normalize_choice(x: str) -> str:\n",
        "    x = x.strip().upper()\n",
        "    for ch in x:\n",
        "        if \"A\" <= ch <= \"Z\":\n",
        "            return ch\n",
        "    return x\n",
        "\n",
        "# 인덱스 리셋이 포인트\n",
        "gt_norm = gt.apply(normalize_choice).reset_index(drop=True)\n",
        "pred_norm = pred.apply(normalize_choice).reset_index(drop=True)\n",
        "\n",
        "print(len(gt_norm), len(pred_norm))  # 둘 다 25 나오는지 체크 한번 해보고\n",
        "\n",
        "# 6. 정확도 계산\n",
        "correct = (gt_norm == pred_norm)\n",
        "accuracy = correct.mean()\n",
        "\n",
        "print(f\"총 문제 수: {len(gt_norm)}\")\n",
        "print(f\"정답 개수: {correct.sum()}\")\n",
        "print(f\"정확도: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4s43TAo8Dry",
        "outputId": "765c63f4-a260-4c9b-fbab-3eae4c3b4c9c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24 24\n",
            "총 문제 수: 24\n",
            "정답 개수: 19\n",
            "정확도: 79.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghx2UmlgJ2bv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}