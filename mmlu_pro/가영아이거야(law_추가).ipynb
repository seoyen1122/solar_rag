{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyen1122/solar_rag/blob/main/mmlu_pro/%EA%B0%80%EC%98%81%EC%95%84%EC%9D%B4%EA%B1%B0%EC%95%BC(law_%EC%B6%94%EA%B0%80).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-6OW5UE6tali",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91fa36d-941b-4498-d1c4-287ec9d4def7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "transformers 4.57.1 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install -qU python-dotenv PyPDF2 langchain langchain-community langchain-core langchain-text-splitters langchain_upstage oracledb python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LEsWxsJjiTqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2dfa7f-8373-42ac-8a0d-2542f7c54d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q openai langchain tiktoken faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TCh-CfFpya_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9822d929-64c0-4d6b-9b9d-13fca10bbd03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2mTZZU4Ps3nU"
      },
      "outputs": [],
      "source": [
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ookX6xz1tCDj"
      },
      "outputs": [],
      "source": [
        "UPSTAGE_API_KEY = \"up_g7T2cQoLKZH6Oi2n4MHOW706XAdSs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hB2GFt5Stg1a"
      },
      "outputs": [],
      "source": [
        "\n",
        "upstage_embeddings = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=\"embedding-passage\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/2025_2/nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6OQFdyOy0jb",
        "outputId": "26a7a010-4d83-4cf1-99e2-4ecec6e60d47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/2025_2/nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avXEWwZbGM3f"
      },
      "source": [
        "# Cornell.edu\n",
        "**알파벳 전체를 돌면서 URL 리스트 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZA5T-y8yuBCH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import string\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u3tKaY5gGMEJ"
      },
      "outputs": [],
      "source": [
        "base = \"https://www.law.cornell.edu\"\n",
        "index_template = base + \"/wex/all/{letter}\"\n",
        "\n",
        "header = {\"User-Agent\": \"law/0.1 (for project; contact:seoyen1122@gmail.com)\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L9jsf1-EGlpC"
      },
      "outputs": [],
      "source": [
        "def collect_links(letters=None):\n",
        "  if letters is None:\n",
        "    letters = list(string.ascii_lowercase)\n",
        "\n",
        "  seen = set()\n",
        "  entries = []\n",
        "\n",
        "  for letter in letters:\n",
        "    index_url = index_template.format(letter = letter)\n",
        "    resp = requests.get(index_url, headers = header) #requests- 웹사이트에 요청보내고응답받기, index_url 주소로 httpget 요청 보냄, header = 나는 이런 이름의 봇이고 프로젝트 목적으로 요청 보낸다는 정보 전달용, return: request.Response객체, 이 페이지 정보, resp.status_code, resp.text, resp.content\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\") #다운받은 html문자열을 파싱해서 태그들을 쉽게 찾을 수 있는 객체로 바꾸는 코드.\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True): #이 페이지 안에서 href 속성이 있는 모든 <a> 태그를 찾아라 → 그 안에 들어있는 링크 주소(href)를 하나씩 꺼내자\n",
        "      href = a[\"href\"]\n",
        "\n",
        "      if not href.startswith(\"/wex/\"):\n",
        "        continue\n",
        "      if \"/wex/all\" in href:\n",
        "        continue\n",
        "\n",
        "      full_url = urljoin(base, href)\n",
        "      if full_url in seen:\n",
        "        continue\n",
        "\n",
        "      term = a.get_text(strip=True)\n",
        "      entries.append({\"term\": term, \"url\":full_url})\n",
        "      seen.add(full_url)\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "  print(\"total entries:\", len(entries))\n",
        "  return entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LEIf_pGKJ9c"
      },
      "source": [
        "**각 항목 페이지에서 본문 텍스트 추출**\n",
        "\n",
        "상단 네비게이션\n",
        "\n",
        "h1 제목(용어)\n",
        "\n",
        "그 아래 여러 단락/리스트가 정의/설명\n",
        "\n",
        "맨 아래에 wex toolbox, terms of use같은 공통 푸터\n",
        "\n",
        "main 안에 있는 p, ul, ol 태그 텍스트를 합치는 식으로 본문만 자르기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j93NfgVAJ6BU"
      },
      "outputs": [],
      "source": [
        "def fetch_article(url):\n",
        "  resp = requests.get(url, headers = header)\n",
        "  resp.raise_for_status()\n",
        "  soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "  main = soup.find(\"main\") or soup\n",
        "\n",
        "  #제목(h1/h2)\n",
        "  title_tag = main.find([\"h1\", \"h2\"])\n",
        "  title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "\n",
        "  #푸터/툴박스 부분 대충 제거 (string 에 \"Wex Toolbox\"가 들어가면 그 뒤는 날려버기)\n",
        "  toolbox = main.find(string=lambda s: isinstance(s, str) and \"Wex Toolbox\" in s)\n",
        "  if toolbox:\n",
        "    parent = toolbox.parent\n",
        "    sib = parent.next_sibling\n",
        "    while sib is not None:\n",
        "      next_sib = sib.next_sibling\n",
        "      try:\n",
        "        sib.decompose()\n",
        "      except Exception:\n",
        "        pass\n",
        "      sib = next_sib\n",
        "\n",
        "  chunks = []\n",
        "  current_title = title or \"Definition\"\n",
        "  current_texts = []\n",
        "\n",
        "  for element in main.find_all([\"h2\", \"p\", \"li\"], recursive=True):\n",
        "    if element.name == \"h2\":\n",
        "      if current_texts:\n",
        "        text = \"\\n\\n\".join(current_texts).strip()\n",
        "        if text:\n",
        "          chunks.append({\n",
        "              \"section_title\": current_title,\n",
        "              \"text\": text\n",
        "          })\n",
        "        current_texts = []\n",
        "      current_title = element.get_text(\" \", strip=True) or title\n",
        "\n",
        "    elif element.name in [\"p\", \"li\"]:\n",
        "      txt = element.get_text(\" \", strip=True)\n",
        "      if txt:\n",
        "        current_texts.append(txt)\n",
        "\n",
        "  if current_texts:\n",
        "    text = \"\\n\\n\".join(current_texts).strip()\n",
        "    if text:\n",
        "      chunks.append({\n",
        "          \"section_title\": current_title,\n",
        "          \"text\": text\n",
        "      })\n",
        "\n",
        "\n",
        "  # 혹시 아무 청크도 못 만들었으면, 전체 p/li를 한 덩어리로라도 넣기 (optional)\n",
        "  if not chunks:\n",
        "      texts = []\n",
        "      for element in main.find_all([\"p\", \"li\"]):\n",
        "          txt = element.get_text(\" \", strip=True)\n",
        "          if txt:\n",
        "              texts.append(txt)\n",
        "      if texts:\n",
        "          chunks.append({\n",
        "              \"section_title\": title or \"Definition\",\n",
        "              \"text\": \"\\n\\n\".join(texts)\n",
        "          })\n",
        "\n",
        "    # 최종 메타데이터 dict 반환\n",
        "  meta = {\n",
        "      \"source_url\": url,\n",
        "      \"title\": title,\n",
        "      \"doc_type\": \"wex\",\n",
        "      \"chunk_list\": chunks,\n",
        "  }\n",
        "  return meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysedD44YL_TT"
      },
      "source": [
        "**전체 크롤링 + jsonl로 저장**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gJd8xDAXL7xU"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VQIYk2lwMF3r"
      },
      "outputs": [],
      "source": [
        "def crawl(output_path=\"wex_raw.jsonl\", letters=None):\n",
        "  entries = collect_links(letters)\n",
        "\n",
        "  with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, entry in enumerate(entries):\n",
        "      url = entry[\"url\"]\n",
        "      meta = fetch_article(url)\n",
        "\n",
        "      if meta.get(\"chunk_list\"):\n",
        "        f.write(json.dumps(meta, ensure_ascii=False) + \"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "      time.sleep(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 판례"
      ],
      "metadata": {
        "id": "Oxw2vEafRjvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case_index_url = base + \"/supct/cases/name.htm\""
      ],
      "metadata": {
        "id": "Wkimm3iTRmAQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_case_links():\n",
        "\n",
        "    resp = requests.get(case_index_url, headers=header)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    entries = []\n",
        "    seen = set()\n",
        "\n",
        "    # 이 페이지에는 A~Z 리스트 안에 <a>들이 잔뜩 있음\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "\n",
        "        # Cornell historic decisions URL 패턴들 (두 가지 정도 있음)\n",
        "        if not (href.startswith(\"/supct/html/\") or href.startswith(\"/supremecourt/text/\")):\n",
        "            continue\n",
        "\n",
        "        full_url = urljoin(base, href)\n",
        "        if full_url in seen:\n",
        "            continue\n",
        "\n",
        "        title = a.get_text(strip=True)\n",
        "        entries.append({\"title\": title, \"url\": full_url})\n",
        "        seen.add(full_url)\n",
        "\n",
        "    print(\"total historic SCOTUS cases:\", len(entries))\n",
        "    return entries"
      ],
      "metadata": {
        "id": "PNmknK6GRqvB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**판례case fetch**"
      ],
      "metadata": {
        "id": "klb60jFdLM46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_case_article(url):\n",
        "    \"\"\"\n",
        "    판례 페이지에서:\n",
        "    - title (사건명)\n",
        "    - Facts / Issue / Holding / Reasoning 등 섹션을 찾아서\n",
        "    Wex와 동일한 스키마로 meta를 반환.\n",
        "    \"\"\"\n",
        "    resp = requests.get(url, headers=header)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # 사이트에 맞게 main 영역 / content 영역 선택\n",
        "    main = soup.find(\"main\") or soup\n",
        "\n",
        "    # 제목\n",
        "    title_tag = main.find([\"h1\", \"h2\"])\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    # (예시) 섹션 제목이 h2/h3 로 되어 있다고 가정\n",
        "    current_title = \"Facts\"\n",
        "    current_texts = []\n",
        "\n",
        "    for element in main.find_all([\"h2\", \"h3\", \"p\", \"li\"], recursive=True):\n",
        "        if element.name in [\"h2\", \"h3\"]:\n",
        "            # 새 섹션 시작 → 이전 섹션 flush\n",
        "            if current_texts:\n",
        "                text = \"\\n\\n\".join(current_texts).strip()\n",
        "                if text:\n",
        "                    chunks.append({\n",
        "                        \"section_title\": current_title,\n",
        "                        \"text\": text,\n",
        "                    })\n",
        "                current_texts = []\n",
        "            current_title = element.get_text(\" \", strip=True) or current_title\n",
        "\n",
        "        elif element.name in [\"p\", \"li\"]:\n",
        "            txt = element.get_text(\" \", strip=True)\n",
        "            if txt:\n",
        "                current_texts.append(txt)\n",
        "\n",
        "    if current_texts:\n",
        "        text = \"\\n\\n\".join(current_texts).strip()\n",
        "        if text:\n",
        "            chunks.append({\n",
        "                \"section_title\": current_title,\n",
        "                \"text\": text,\n",
        "            })\n",
        "\n",
        "    # fallback: 그래도 없으면 전체 p/li 한번에\n",
        "    if not chunks:\n",
        "        texts = []\n",
        "        for element in main.find_all([\"p\", \"li\"]):\n",
        "            txt = element.get_text(\" \", strip=True)\n",
        "            if txt:\n",
        "                texts.append(txt)\n",
        "        if texts:\n",
        "            chunks.append({\n",
        "                \"section_title\": title or \"Opinion\",\n",
        "                \"text\": \"\\n\\n\".join(texts),\n",
        "            })\n",
        "\n",
        "    meta = {\n",
        "        \"source_url\": url,\n",
        "        \"title\": title,\n",
        "        \"doc_type\": \"case\",\n",
        "        \"chunk_list\": chunks,\n",
        "    }\n",
        "    return meta"
      ],
      "metadata": {
        "id": "837-X0WrLJvO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**판례case 크롤링**"
      ],
      "metadata": {
        "id": "AYCoH5bDLX0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_cases(output_path=\"cases_structured_all.jsonl\", case_urls=None):\n",
        "    \"\"\"\n",
        "    case_urls: 미리 수집해둔 판례 URL 리스트 (LII/Oyez 등)\n",
        "    \"\"\"\n",
        "    if case_urls is None:\n",
        "        entries = collect_case_links()\n",
        "        case_urls = [e[\"url\"] for e in entries]\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, url in enumerate(case_urls):\n",
        "            try:\n",
        "                meta = fetch_case_article(url)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] failed to fetch {url}: {e}\")\n",
        "                continue\n",
        "\n",
        "            if meta.get(\"chunk_list\"):\n",
        "                f.write(json.dumps(meta, ensure_ascii=False) + \"\\n\")\n",
        "                f.flush()\n",
        "\n",
        "            time.sleep(3)\n",
        "\n",
        "    print(\"cases saved to\", output_path)"
      ],
      "metadata": {
        "id": "LXf27KM0LcuI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItivvmZkOb94"
      },
      "source": [
        "\n",
        "\n",
        "**청킹 + 임베딩 + FAISS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5RcEcVe6PfTj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = UPSTAGE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "V8HnJ0Y1OaYs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_upstage import UpstageEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RjXh0WkmPt58"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_chars=800, overlap_chars=200):\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  n = len(text)\n",
        "  while start < n:\n",
        "    end = min(start + max_chars, n)\n",
        "    chunk = text[start:end]\n",
        "    chunks.append(chunk)\n",
        "    if end == n:\n",
        "      break\n",
        "    start = end - overlap_chars\n",
        "  return chunks\n",
        "\n",
        "def load_chunks_multi(jsonl_paths, max_chars=None, overlap_chars=200):\n",
        "    chunks = []\n",
        "    metadata = []\n",
        "\n",
        "    for jsonl_path in jsonl_paths:\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                rec = json.loads(line)\n",
        "                source_url = rec[\"source_url\"]\n",
        "                title = rec[\"title\"]\n",
        "                doc_type = rec.get(\"doc_type\", \"wex\")  # ★ Wex vs case 구분\n",
        "                chunk_list = rec.get(\"chunk_list\", [])\n",
        "\n",
        "                for sec_idx, c in enumerate(chunk_list):\n",
        "                    section_title = c.get(\"section_title\", title)\n",
        "                    text = c.get(\"text\", \"\")\n",
        "                    if not text.strip():\n",
        "                        continue\n",
        "\n",
        "                    if max_chars is None:\n",
        "                        chunks.append(text)\n",
        "                        metadata.append({\n",
        "                            \"source_url\": source_url,\n",
        "                            \"title\": title,\n",
        "                            \"doc_type\": doc_type,\n",
        "                            \"section_title\": section_title,\n",
        "                            \"section_index\": sec_idx,\n",
        "                            \"subchunk_index\": 0,\n",
        "                        })\n",
        "                    else:\n",
        "                        for sub_idx, ch in enumerate(\n",
        "                            chunk_text(text, max_chars, overlap_chars)\n",
        "                        ):\n",
        "                            chunks.append(ch)\n",
        "                            metadata.append({\n",
        "                                \"source_url\": source_url,\n",
        "                                \"title\": title,\n",
        "                                \"doc_type\": doc_type,\n",
        "                                \"section_title\": section_title,\n",
        "                                \"section_index\": sec_idx,\n",
        "                                \"subchunk_index\": sub_idx,\n",
        "                            })\n",
        "\n",
        "    return chunks, metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_faiss_index(\n",
        "    jsonl_paths = (\"wex_structured_all.jsonl\", \"cases_structured_all.jsonl\"),\n",
        "    index_path = \"law_faiss.index\",\n",
        "    meta_path = \"law_metadata.jsonl\",\n",
        "):\n",
        "    chunks, metadata = load_chunks_multi(jsonl_paths, max_chars=800, overlap_chars=200)\n",
        "    print(\"num_chunks:\", len(chunks))\n",
        "\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "\n",
        "    vecs = embeddings.embed_documents(chunks)\n",
        "    emb = np.array(vecs, dtype=np.float32)\n",
        "    print(\"emb shape\", emb.shape)\n",
        "\n",
        "    dim = emb.shape[1]\n",
        "    faiss.normalize_L2(emb)\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(emb)\n",
        "\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for m in metadata:\n",
        "            f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(\"index saved to\", index_path)\n",
        "    print(\"metadata saved to\", meta_path)"
      ],
      "metadata": {
        "id": "fAEjNyybToCF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Wex 크롤링 → wex_structured_all.jsonl 생성\n",
        "crawl(output_path=\"wex_structured_all.jsonl\", letters=None)\n",
        "\n",
        "# 2) 판례 크롤링 → cases_structured_all.jsonl 생성\n",
        "crawl_cases(output_path=\"cases_structured_all.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "Ls45DRFDT0MV",
        "outputId": "9029f862-bcc7-4b7a-ac77-d43749d183a8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2296774448.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1) Wex 크롤링 → wex_structured_all.jsonl 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wex_structured_all.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2) 판례 크롤링 → cases_structured_all.jsonl 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcrawl_cases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cases_structured_all.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2348373356.py\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(output_path, letters)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wex_raw.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4248028855.py\u001b[0m in \u001b[0;36mcollect_links\u001b[0;34m(letters)\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total entries:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) 두 JSONL을 함께 써서 Law FAISS 인덱스 생성\n",
        "build_faiss_index(\n",
        "    jsonl_paths=(\"wex_structured_all.jsonl\", \"cases_structured_all.jsonl\"),\n",
        "    index_path=\"law_faiss.index\",\n",
        "    meta_path=\"law_metadata.jsonl\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf5wygbUNcGR",
        "outputId": "69085f1a-2d3f-4456-e717-0d96d7a6505f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_chunks: 105996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2y5OJiYoz9X"
      },
      "outputs": [],
      "source": [
        "from langchain_upstage import UpstageEmbeddings\n",
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "\n",
        "def build_faiss_index(\n",
        "    jsonl_path=[\"wex_structured_all.jsonl\", \"cases_structured_all.jsonl\"],\n",
        "    index_path=\"law_faiss.index\",\n",
        "    meta_path=\"law_metadata.jsonl\",\n",
        "):\n",
        "    # 1) 청크 + 메타데이터 로드\n",
        "    chunks, metadata = load_chunks_multi(jsonl_path, max_chars=800)\n",
        "    print(\"num_chunks:\", len(chunks))\n",
        "\n",
        "    # 2) Upstage 임베딩 모델 (★ suffix 빼고 base 이름만 사용)\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "\n",
        "    # 3) 배치로 나눠서 embed_documents 호출 (예: 256개씩)\n",
        "    all_vecs = []\n",
        "    batch_size = 256\n",
        "\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i : i + batch_size]\n",
        "        # 혹시라도 공백 문자열이 섞여 있으면 제거 (안전)\n",
        "        batch = [c for c in batch if c.strip()]\n",
        "        if not batch:\n",
        "            continue\n",
        "\n",
        "        vecs = embeddings.embed_documents(batch)\n",
        "        all_vecs.extend(vecs)\n",
        "        print(f\"embedded {i + len(batch)} / {len(chunks)}\")\n",
        "\n",
        "    emb = np.array(all_vecs, dtype=np.float32)\n",
        "    print(\"emb shape\", emb.shape)\n",
        "\n",
        "    dim = emb.shape[1]\n",
        "    faiss.normalize_L2(emb)\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(emb)\n",
        "\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for m in metadata:\n",
        "            f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(\"index saved to\", index_path)\n",
        "    print(\"metadata saved to\", meta_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "build_faiss_index(\n",
        "    jsonl_path=\"wex_structured_all.jsonl\",\n",
        "    index_path=\"wex_faiss_all.index\",\n",
        "    meta_path=\"wex_metadata_all.jsonl\",\n",
        ")"
      ],
      "metadata": {
        "id": "SOeJI974vkWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jsonlines"
      ],
      "metadata": {
        "id": "A0ylZnu2-qW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "create_faiss_index.py\n",
        "\n",
        "- 'entries.jsonl' (Semantic Chunking된 파일)을 읽어옵니다.\n",
        "- 각 청크(섹션)를 로드합니다.\n",
        "- (안전 장치) 만약 섹션 텍스트가 1000자를 넘으면, 1000자 단위로 더 잘게 자릅니다.\n",
        "- 이 과정에서 'title', 'source_url', 'section_title' 메타데이터를 모두 보존합니다.\n",
        "- BGE-small 임베딩 모델을 사용하여 모든 청크를 임베딩합니다.\n",
        "- 'faiss_index_philosophy'라는 이름의 로컬 FAISS 인덱스로 저장합니다.\n",
        "\"\"\"\n",
        "\n",
        "import jsonlines\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. 설정값\n",
        "# ----------------------------------------------------\n",
        "JSONL_FILE = [\"wex_structured_all.jsonl\", \"cases_structured_all.jsonl\"]           # 입력 파일 (스크래핑 결과)\n",
        "INDEX_NAME = \"faiss_index_law\"    # 저장할 FAISS 인덱스 이름\n",
        "\n",
        "# \"Safety Net\" 청킹 설정 (H2 섹션이 너무 클 경우 대비)\n",
        "CHUNK_SIZE = 1000   # 청크 최대 글자 수\n",
        "CHUNK_OVERLAP = 100 # 청크 겹침\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. 임베딩 모델 로드\n",
        "# ----------------------------------------------------\n",
        "try:\n",
        "    upstage_embeddings\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model. Do you have a GPU runtime? {e}\")\n",
        "    # (GPU 런타임이 아닐 경우 CPU로 fallback)\n",
        "    # model_kwargs = {'device': 'cpu'}\n",
        "    # embedding_model = HuggingFaceEmbeddings(\n",
        "    #     model_name=model_name,\n",
        "    #     model_kwargs=model_kwargs,\n",
        "    #     encode_kwargs=encode_kwargs\n",
        "    # )\n",
        "print(\"Embedding model loaded.\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. JSONL 로드 및 '안전 장치' 청킹\n",
        "# ----------------------------------------------------\n",
        "print(f\"Loading '{JSONL_FILE}' and applying safety net chunking...\")\n",
        "\n",
        "# '안전 장치' (H2 섹션이 너무 클 경우를 대비한) 텍스트 분할기\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "all_final_chunks = [] # 최종적으로 FAISS에 들어갈 Document 객체 리스트\n",
        "\n",
        "try:\n",
        "    with jsonlines.open(JSONL_FILE, 'r') as reader:\n",
        "        for entry in reader:\n",
        "            # (1) 기본 메타데이터 (페이지 레벨)\n",
        "            base_metadata = {\n",
        "                \"source\": entry.get(\"source_url\", \"N/A\"),\n",
        "                \"title\": entry.get(\"title\", \"N/A\"),\n",
        "                \"doc_type\": entry.get(\"doc_type\", \"wex\"),\n",
        "            }\n",
        "\n",
        "            # (2) Semantic Chunking된 'chunk_list' 순회\n",
        "            for chunk in entry.get(\"chunk_list\", []):\n",
        "                section_text = chunk.get(\"text\")\n",
        "                section_title = chunk.get(\"section_title\", \"N/A\")\n",
        "\n",
        "                if not section_text:\n",
        "                    continue\n",
        "\n",
        "                # (3) H2 섹션 텍스트가 CHUNK_SIZE(1000자)를 넘을 경우,\n",
        "                #     text_splitter가 이 텍스트를 더 작은 '미니 청크'로 자름\n",
        "                split_texts = text_splitter.split_text(section_text)\n",
        "\n",
        "                # (4) 이 '미니 청크'들을 Document 객체로 변환\n",
        "                for text_piece in split_texts:\n",
        "                    # 메타데이터에 'section' 정보를 추가\n",
        "                    final_metadata = base_metadata.copy()\n",
        "                    final_metadata[\"section\"] = section_title\n",
        "\n",
        "                    new_doc = Document(page_content=text_piece, metadata=final_metadata)\n",
        "                    all_final_chunks.append(new_doc)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{JSONL_FILE}' not found. Please run the scraping script first.\")\n",
        "    sys.exit()\n",
        "\n",
        "print(f\"Total 'mini-chunks' to be indexed: {len(all_final_chunks)}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. FAISS 임베딩 및 저장\n",
        "# ----------------------------------------------------\n",
        "if all_final_chunks:\n",
        "    print(\"Starting FAISS index creation... (This may take a long time)\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # FAISS.from_documents()를 사용하면\n",
        "    # 텍스트 청크는 임베딩되고, 메타데이터는 그대로 벡터 스토어에 저장됩니다.\n",
        "    db_psychology = FAISS.from_documents(all_final_chunks, upstage_embeddings)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"FAISS index created successfully in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 생성된 인덱스를 파일로 저장\n",
        "    db_psychology.save_local(INDEX_NAME)\n",
        "\n",
        "    print(f\"FAISS index saved to folder: '{INDEX_NAME}'\")\n",
        "else:\n",
        "    print(\"No chunks were created. FAISS index not built.\")"
      ],
      "metadata": {
        "id": "LXEibfqWwCTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11/23 판례 사이트 정보 추가"
      ],
      "metadata": {
        "id": "OtkuTq9FJ2Sw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GpAg6Z8AG45fWZlznCuVYesm5fOYDUIE",
      "authorship_tag": "ABX9TyNOF2CTWNwOJv4KNaiXUtGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}