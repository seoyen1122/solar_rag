{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "1Uv6ggol9GTYjCiXeJkvqYOIvzXrfZuHS",
      "authorship_tag": "ABX9TyPKpBLOwS0kCF+yUh3mz+qX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyen1122/solar_rag/blob/main/mmlu_pro/psychology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q openai langchain tiktoken faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaWj2RiOg6Q-",
        "outputId": "633d86b6-6e09-4215-c7c8-20aa9bd134b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NZq470lWZ8l",
        "outputId": "a3b17cbd-2d87-4401-b665-b5d17b521145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-upstage\n",
            "  Downloading langchain_upstage-0.7.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.78 in /usr/local/lib/python3.12/dist-packages (from langchain-upstage) (0.3.79)\n",
            "Collecting langchain-openai<0.4.0,>=0.3.34 (from langchain-upstage)\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from langchain-upstage) (2.32.4)\n",
            "Collecting tokenizers<0.21.0,>=0.20.0 (from langchain-upstage)\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.31.0 (from langchain-upstage)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.42)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.78->langchain-upstage) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.78->langchain-upstage) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.78->langchain-upstage) (25.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai<0.4.0,>=0.3.34->langchain-upstage) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai<0.4.0,>=0.3.34->langchain-upstage) (0.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers<0.21.0,>=0.20.0->langchain-upstage) (0.36.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<0.4.0,>=0.3.78->langchain-upstage) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4.0,>=0.3.34->langchain-upstage) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4.0,>=0.3.34->langchain-upstage) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4.0,>=0.3.34->langchain-upstage) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai<0.4.0,>=0.3.34->langchain-upstage) (2024.11.6)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_upstage-0.7.4-py3-none-any.whl (25 kB)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, typing-inspect, tokenizers, dataclasses-json, langchain-openai, langchain-upstage, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "transformers 4.57.1 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.31 langchain-openai-0.3.35 langchain-upstage-0.7.4 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-4.3.1 requests-2.32.5 tokenizers-0.20.3 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-upstage langchain-community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UPSTAGE_API_KEY = \"up_g7T2cQoLKZH6Oi2n4MHOW706XAdSs\""
      ],
      "metadata": {
        "id": "ROkhTCTvr6_9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = UPSTAGE_API_KEY"
      ],
      "metadata": {
        "id": "hGcZVjsUrt7r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_upstage import UpstageEmbeddings"
      ],
      "metadata": {
        "id": "yKLdGJyCr-Ie"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHAPTERS = [\n",
        "    {\"num\": 1, \"title\": \"Introduction to Psychology\", \"start\": 7,  \"end\": 34},\n",
        "    {\"num\": 2, \"title\": \"Psychological Research\",     \"start\": 35, \"end\": 70},\n",
        "    {\"num\": 3, \"title\": \"Biopsychology\",              \"start\": 71, \"end\": 108},\n",
        "    {\"num\": 4, \"title\": \"States of Consciousness\",    \"start\": 109, \"end\": 143},\n",
        "    {\"num\": 5, \"title\": \"Sensation and Perception\",    \"start\": 145, \"end\": 179},\n",
        "    {\"num\": 6, \"title\": \"Learning\",    \"start\": 181, \"end\": 211},\n",
        "    {\"num\": 7, \"title\": \"Thinking and Intelligence\",    \"start\": 213, \"end\": 246},\n",
        "    {\"num\": 8, \"title\": \"Memory\",    \"start\": 247, \"end\": 277},\n",
        "    {\"num\": 9, \"title\": \"Livespan Development\",    \"start\": 279, \"end\": 320},\n",
        "    {\"num\": 10, \"title\": \"Emotion and Motivation\",    \"start\": 321, \"end\": 357},\n",
        "    {\"num\": 11, \"title\": \"Personality\",    \"start\": 359, \"end\": 397},\n",
        "    {\"num\": 12, \"title\": \"Social Psychology\",    \"start\": 399, \"end\": 444},\n",
        "    {\"num\": 13, \"title\": \"Industrial-Organizational Psychology\",    \"start\": 447, \"end\": 484},\n",
        "    {\"num\": 14, \"title\": \"Stress, Lifestyle, and Health\",    \"start\": 485, \"end\": 535},\n",
        "    {\"num\": 15, \"title\": \"Psychological Disorders\",    \"start\": 537, \"end\": 598},\n",
        "    {\"num\": 16, \"title\": \"Therapy and Treatment\",    \"start\": 599, \"end\": 733},\n",
        "]"
      ],
      "metadata": {
        "id": "B_2NfaeSu4hq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/2025_2/nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSEwASPae1G0",
        "outputId": "1ccda726-8583-41f3-bf64-6ea27d866249"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/2025_2/nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjEzIOIwFXj6",
        "outputId": "2b811bfc-660b-45cf-fff2-b31921f5bffc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.4.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import jsonlines\n",
        "from pypdf import PdfReader"
      ],
      "metadata": {
        "id": "b29lxWaxekLz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_PATH = \"psychology2e.pdf\"\n",
        "OUTPUT_JSONL = \"entries_psychology.jsonl\"\n",
        "BASE_URL = \"https://openstax.org/books/psychology-2e\""
      ],
      "metadata": {
        "id": "Q62Ey5S9FWye"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pages_text(reader: PdfReader, start_page: int, end_page: int) -> str:\n",
        "    texts = []\n",
        "    for i in range(start_page, end_page):\n",
        "        if i < 0 or i >= len(reader.pages):\n",
        "            continue\n",
        "        try:\n",
        "            t = reader.pages[i].extract_text() or \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] page {i} extract_text 실패: {e}\")\n",
        "            t = \"\"\n",
        "        if t.strip():\n",
        "            texts.append(t)\n",
        "    return \"\\n\".join(texts)"
      ],
      "metadata": {
        "id": "l9zygZPRFiLG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_whitespace(text: str) -> str:\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
        "    # 연속 공백 줄이기\n",
        "    text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "1W2MQOXPFowv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_chapter_into_sections(ch_num: int, chapter_text: str):\n",
        "    \"\"\"\n",
        "    한 챕터 텍스트를 섹션(1.1, 1.2, ..., Key Terms, Summary, Review...) 단위로 나눔.\n",
        "\n",
        "    반환: [{\"section_title\": \"...\", \"text\": \"...\"}, ...]\n",
        "    \"\"\"\n",
        "    # 줄 단위로 먼저 나눔\n",
        "    text = normalize_whitespace(chapter_text)\n",
        "    lines = text.splitlines()\n",
        "\n",
        "    # 섹션 헤더 패턴: \"1.1 What Is Psychology?\" 같이 챕터번호로 시작하는 제목\n",
        "    # + 요약/연습문제 섹션들 (필요에 따라 추가)\n",
        "    header_regex = re.compile(\n",
        "        rf\"^(?:{ch_num}\\.\\d+\\s+.+|Key Terms|Summary|Review Questions|Critical Thinking Questions|Personal Application Questions)\\s*$\"\n",
        "    )\n",
        "\n",
        "    # 헤더 위치 찾기\n",
        "    headers = []\n",
        "    for idx, line in enumerate(lines):\n",
        "        if header_regex.match(line.strip()):\n",
        "            headers.append((idx, line.strip()))\n",
        "\n",
        "    if not headers:\n",
        "        # 헤더 못 찾으면 챕터 전체를 하나의 섹션으로 반환\n",
        "        print(f\"[WARN] chapter {ch_num} 에서 섹션 헤더를 찾지 못했습니다. 전체를 1개로 처리.\")\n",
        "        return [{\n",
        "            \"section_title\": f\"Chapter {ch_num} (full)\",\n",
        "            \"text\": text,\n",
        "        }]\n",
        "\n",
        "    sections = []\n",
        "    for i, (line_idx, header_text) in enumerate(headers):\n",
        "        start_line = line_idx + 1  # 헤더 다음 줄부터 내용 시작\n",
        "        end_line = headers[i + 1][0] if i + 1 < len(headers) else len(lines)\n",
        "\n",
        "        body_lines = lines[start_line:end_line]\n",
        "        body = \"\\n\".join(body_lines).strip()\n",
        "\n",
        "        # body가 너무 짧으면 헤더 뒤 내용이 없는 경우 → 그냥 스킵할 수도 있음\n",
        "        if not body:\n",
        "            continue\n",
        "\n",
        "        sections.append({\n",
        "            \"section_title\": header_text,\n",
        "            \"text\": body,\n",
        "        })\n",
        "\n",
        "    return sections"
      ],
      "metadata": {
        "id": "dHeygfh4Fqek"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    pdf_path = Path(PDF_PATH)\n",
        "    assert pdf_path.exists(), f\"PDF가 없습니다: {pdf_path}\"\n",
        "\n",
        "    reader = PdfReader(str(pdf_path))\n",
        "\n",
        "    with jsonlines.open(OUTPUT_JSONL, mode=\"w\") as writer:\n",
        "        for ch in CHAPTERS:\n",
        "            ch_num = ch[\"num\"]\n",
        "            ch_title = ch[\"title\"]\n",
        "            start = ch[\"start\"]\n",
        "            end = ch[\"end\"]\n",
        "\n",
        "            print(f\"[CHAPTER {ch_num}] {ch_title} (pages {start}~{end})\")\n",
        "\n",
        "            chapter_text = extract_pages_text(reader, start, end)\n",
        "            sections = split_chapter_into_sections(ch_num, chapter_text)\n",
        "            print(f\"  - 섹션 개수: {len(sections)}\")\n",
        "\n",
        "            entry = {\n",
        "                \"source_url\": f\"{BASE_URL}#chapter-{ch_num}\",\n",
        "                \"title\": f\"Chapter {ch_num} {ch_title}\",\n",
        "                \"chunk_list\": sections,   # [{section_title, text}, ...]\n",
        "            }\n",
        "\n",
        "            writer.write(entry)\n",
        "\n",
        "    print(f\"[완료] {OUTPUT_JSONL} 에 chapter/section 단위 entries 저장 완료.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcHSuqGMFtUz",
        "outputId": "626a6edb-a77d-4bbe-ba1d-68b6154fb0c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHAPTER 1] Introduction to Psychology (pages 7~34)\n",
            "  - 섹션 개수: 6\n",
            "[CHAPTER 2] Psychological Research (pages 35~70)\n",
            "  - 섹션 개수: 11\n",
            "[CHAPTER 3] Biopsychology (pages 71~108)\n",
            "  - 섹션 개수: 11\n",
            "[CHAPTER 4] States of Consciousness (pages 109~143)\n",
            "  - 섹션 개수: 12\n",
            "[CHAPTER 5] Sensation and Perception (pages 145~179)\n",
            "  - 섹션 개수: 12\n",
            "[CHAPTER 6] Learning (pages 181~211)\n",
            "  - 섹션 개수: 10\n",
            "[CHAPTER 7] Thinking and Intelligence (pages 213~246)\n",
            "  - 섹션 개수: 6\n",
            "[CHAPTER 8] Memory (pages 247~277)\n",
            "  - 섹션 개수: 9\n",
            "[CHAPTER 9] Livespan Development (pages 279~320)\n",
            "  - 섹션 개수: 12\n",
            "[CHAPTER 10] Emotion and Motivation (pages 321~357)\n",
            "  - 섹션 개수: 12\n",
            "[CHAPTER 11] Personality (pages 359~397)\n",
            "  - 섹션 개수: 18\n",
            "[CHAPTER 12] Social Psychology (pages 399~444)\n",
            "  - 섹션 개수: 13\n",
            "[CHAPTER 13] Industrial-Organizational Psychology (pages 447~484)\n",
            "  - 섹션 개수: 13\n",
            "[CHAPTER 14] Stress, Lifestyle, and Health (pages 485~535)\n",
            "  - 섹션 개수: 13\n",
            "[CHAPTER 15] Psychological Disorders (pages 537~598)\n",
            "  - 섹션 개수: 18\n",
            "[CHAPTER 16] Therapy and Treatment (pages 599~733)\n",
            "  - 섹션 개수: 23\n",
            "[완료] entries_psychology.jsonl 에 chapter/section 단위 entries 저장 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upstage_embeddings = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=\"embedding-passage\")"
      ],
      "metadata": {
        "id": "wfTSpWyyHgzc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "create_faiss_index.py\n",
        "\n",
        "- 'entries.jsonl' (Semantic Chunking된 파일)을 읽어옵니다.\n",
        "- 각 청크(섹션)를 로드합니다.\n",
        "- (안전 장치) 만약 섹션 텍스트가 1000자를 넘으면, 1000자 단위로 더 잘게 자릅니다.\n",
        "- 이 과정에서 'title', 'source_url', 'section_title' 메타데이터를 모두 보존합니다.\n",
        "- BGE-small 임베딩 모델을 사용하여 모든 청크를 임베딩합니다.\n",
        "- 'faiss_index_philosophy'라는 이름의 로컬 FAISS 인덱스로 저장합니다.\n",
        "\"\"\"\n",
        "\n",
        "import jsonlines\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. 설정값\n",
        "# ----------------------------------------------------\n",
        "JSONL_FILE = \"entries_psychology.jsonl\"             # 입력 파일 (스크래핑 결과)\n",
        "INDEX_NAME = \"faiss_index_psychology\"    # 저장할 FAISS 인덱스 이름\n",
        "\n",
        "# \"Safety Net\" 청킹 설정 (H2 섹션이 너무 클 경우 대비)\n",
        "CHUNK_SIZE = 1000   # 청크 최대 글자 수\n",
        "CHUNK_OVERLAP = 100 # 청크 겹침\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. 임베딩 모델 로드 (BGE-m3)\n",
        "# ----------------------------------------------------\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "model_kwargs = {'device': 'cuda'} # Colab GPU 사용\n",
        "encode_kwargs = {'normalize_embeddings': True} # BGE 모델 권장 사항\n",
        "\n",
        "try:\n",
        "    upstage_embeddings\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model. Do you have a GPU runtime? {e}\")\n",
        "    # (GPU 런타임이 아닐 경우 CPU로 fallback)\n",
        "    # model_kwargs = {'device': 'cpu'}\n",
        "    # embedding_model = HuggingFaceEmbeddings(\n",
        "    #     model_name=model_name,\n",
        "    #     model_kwargs=model_kwargs,\n",
        "    #     encode_kwargs=encode_kwargs\n",
        "    # )\n",
        "print(\"Embedding model loaded.\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. JSONL 로드 및 '안전 장치' 청킹\n",
        "# ----------------------------------------------------\n",
        "print(f\"Loading '{JSONL_FILE}' and applying safety net chunking...\")\n",
        "\n",
        "# '안전 장치' (H2 섹션이 너무 클 경우를 대비한) 텍스트 분할기\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "all_final_chunks = [] # 최종적으로 FAISS에 들어갈 Document 객체 리스트\n",
        "\n",
        "try:\n",
        "    with jsonlines.open(JSONL_FILE, 'r') as reader:\n",
        "        for entry in reader:\n",
        "            # (1) 기본 메타데이터 (페이지 레벨)\n",
        "            base_metadata = {\n",
        "                \"source\": entry.get(\"source_url\", \"N/A\"),\n",
        "                \"title\": entry.get(\"title\", \"N/A\"),\n",
        "            }\n",
        "\n",
        "            # (2) Semantic Chunking된 'chunk_list' 순회\n",
        "            for chunk in entry.get(\"chunk_list\", []):\n",
        "                section_text = chunk.get(\"text\")\n",
        "                section_title = chunk.get(\"section_title\", \"N/A\")\n",
        "\n",
        "                if not section_text:\n",
        "                    continue\n",
        "\n",
        "                # (3) H2 섹션 텍스트가 CHUNK_SIZE(1000자)를 넘을 경우,\n",
        "                #     text_splitter가 이 텍스트를 더 작은 '미니 청크'로 자름\n",
        "                split_texts = text_splitter.split_text(section_text)\n",
        "\n",
        "                # (4) 이 '미니 청크'들을 Document 객체로 변환\n",
        "                for text_piece in split_texts:\n",
        "                    # 메타데이터에 'section' 정보를 추가\n",
        "                    final_metadata = base_metadata.copy()\n",
        "                    final_metadata[\"section\"] = section_title\n",
        "\n",
        "                    new_doc = Document(page_content=text_piece, metadata=final_metadata)\n",
        "                    all_final_chunks.append(new_doc)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{JSONL_FILE}' not found. Please run the scraping script first.\")\n",
        "    sys.exit()\n",
        "\n",
        "print(f\"Total 'mini-chunks' to be indexed: {len(all_final_chunks)}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. FAISS 임베딩 및 저장\n",
        "# ----------------------------------------------------\n",
        "if all_final_chunks:\n",
        "    print(\"Starting FAISS index creation... (This may take a long time)\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # FAISS.from_documents()를 사용하면\n",
        "    # 텍스트 청크는 임베딩되고, 메타데이터는 그대로 벡터 스토어에 저장됩니다.\n",
        "    db_philosophy = FAISS.from_documents(all_final_chunks, upstage_embeddings)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"FAISS index created successfully in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 생성된 인덱스를 파일로 저장\n",
        "    db_philosophy.save_local(INDEX_NAME)\n",
        "\n",
        "    print(f\"FAISS index saved to folder: '{INDEX_NAME}'\")\n",
        "else:\n",
        "    print(\"No chunks were created. FAISS index not built.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYeWg8KAFwqn",
        "outputId": "13830d24-e7e6-4d5a-ccc9-dd6b827239a7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model loaded.\n",
            "Loading 'entries_psychology.jsonl' and applying safety net chunking...\n",
            "Total 'mini-chunks' to be indexed: 2232\n",
            "Starting FAISS index creation... (This may take a long time)\n",
            "FAISS index created successfully in 330.44 seconds.\n",
            "FAISS index saved to folder: 'faiss_index_psychology'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgSn8saiHMnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}