{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QPohq1Tz_OyEJyW_0q2GagyUMbUklkyy",
      "authorship_tag": "ABX9TyPfJ0s7nW3VlAlgCTak4eiD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyen1122/solar_rag/blob/main/mmlu_pro/(41)pro%EA%B3%A0%EC%B9%9C%EA%B1%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyH8sMMs2RD-",
        "outputId": "0db799f2-4771-4b4a-845e-3c3ad08decfe",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-upstage\n",
            "  Downloading langchain_upstage-0.7.5-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting langchain-core<2.0.0,>=1.0.3 (from langchain-upstage)\n",
            "  Downloading langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain-openai<2.0.0,>=1.0.2 (from langchain-upstage)\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pypdf<5.0.0,>=4.2.0 (from langchain-upstage)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from langchain-upstage) (2.32.4)\n",
            "Collecting tokenizers<0.21.0,>=0.20.0 (from langchain-upstage)\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.31.0 (from langchain-upstage)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.42)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-upstage) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-upstage) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-upstage) (4.15.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (0.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers<0.21.0,>=0.20.0->langchain-upstage) (0.36.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.3->langchain-upstage) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai<2.0.0,>=1.0.2->langchain-upstage) (2024.11.6)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_upstage-0.7.5-py3-none-any.whl (20 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.7-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m473.0/473.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, typing-inspect, tokenizers, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-upstage, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "transformers 4.57.1 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.7 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.7 langchain-openai-1.0.3 langchain-text-splitters-1.0.0 langchain-upstage-0.7.5 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-4.3.1 requests-2.32.5 tokenizers-0.20.3 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-upstage langchain-community pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pdBgMTNwMuCm",
        "outputId": "f186e5fa-dadb-420e-caf3-60b59727db09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2025.10.5)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=ab3ea66d44e15d1fc65958a42019c5f621e6054180fdf27dd64a89743d9aab0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3c/79/b36253689d838af4a0539782853ac3cc38a83a6591ad570dde\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C-AXD7fLRJI-",
        "outputId": "bb6148b5-5c31-4981-d64a-b363215076ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "\n",
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "import wikipediaapi\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8qWw7Tq17dHw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/2025_2/nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gavGzRrztXz",
        "outputId": "7994eb96-f6e4-4a08-c546-177616eb0e66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/2025_2/nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"testset.csv\")"
      ],
      "metadata": {
        "id": "cgvozRFhCJMz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UPSTAGE_API_KEY = \"up_g7T2cQoLKZH6Oi2n4MHOW706XAdSs\""
      ],
      "metadata": {
        "id": "c-yFVp9vQ26G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_classifier = ChatUpstage(\n",
        "    api_key=UPSTAGE_API_KEY,\n",
        "    model=\"solar-pro2\",\n",
        "    temperature=0.1, #ê±°ì˜ í•­ìƒ ê°™ì€ ê²°ê³¼, ì•ˆì •ì ì´ê²Œ\n",
        ")"
      ],
      "metadata": {
        "id": "iK1l_XYgUPay"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_solver = ChatUpstage(\n",
        "    api_key=UPSTAGE_API_KEY,\n",
        "    model=\"solar-pro2\",\n",
        "    temperature=0.2, #ì¢€ ë” ìœ ì—°í•œ ë‹µë³€ì„ ë‚´ë„ë¡\n",
        ")"
      ],
      "metadata": {
        "id": "oElUzmTxV6M-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_index = {\n",
        "    \"law\":        \"/content/drive/MyDrive/Colab Notebooks/2025_2/nlp/mmlu_category/law\",\n",
        "    \"psychology\": \"/content/drive/MyDrive/Colab Notebooks/2025_2/nlp/mmlu_category/psychology\",\n",
        "    \"business\":   \"/content/drive/MyDrive/Colab Notebooks/2025_2/nlp/mmlu_category/business\",\n",
        "    \"philosophy\": \"/content/drive/MyDrive/Colab Notebooks/2025_2/nlp/mmlu_category/philosophy\",\n",
        "    \"history\":    \"/content/drive/MyDrive/Colab Notebooks/2025_2/nlp/mmlu_category/history\",\n",
        "}\n",
        "categories = list(category_index.keys())"
      ],
      "metadata": {
        "id": "T6fjf7tL0E6A"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=\"solar-embedding-1-large\")\n",
        "\n",
        "# ì¹´í…Œê³ ë¦¬ë³„ faiss í•œë²ˆì— ë¡œë“œí•´ì„œ ìºì‹œ\n",
        "vectorstore = {}\n",
        "for cat, path in category_index.items():\n",
        "    vectorstore[cat] = FAISS.load_local(\n",
        "        folder_path=path,\n",
        "        embeddings=emb,\n",
        "        allow_dangerous_deserialization=True,\n",
        "    )\n",
        "\n",
        "wiki = wikipediaapi.Wikipedia(user_agent= \"NLP-RAG/1.0\", language = 'en')"
      ],
      "metadata": {
        "id": "wvfgJRYeP9M4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1st LLM**:\n",
        "\n",
        "- return category\n",
        "\n",
        "- return 3 keywords"
      ],
      "metadata": {
        "id": "WRn3avDKKexv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_prompt_template = \"\"\"\n",
        "You are an expert exam classifier for MMLU-Pro\n",
        "\n",
        "There are 5 possible categories:\n",
        "- law\n",
        "- psychology\n",
        "- business\n",
        "- philosophy\n",
        "- history\n",
        "\n",
        "[Task]\n",
        "Given a multiple-choice exam question (including all options),\n",
        "1. Choose one best category from the list above.\n",
        "2. Extract exactly 3 important keywords (one noun that consists of single word or short phrase)\n",
        "   that will be useful to search textbooks and Wikipedia.\n",
        "   Keywords should be as specific and accurate as possible (e.g., \"consent\",\n",
        "   \"cognitive dissonance\", \"Keynesian economics\").\n",
        "\n",
        "[Output format]\n",
        "Return a valid JSON with the following fields:\n",
        "- \"category\": one of [\"law\",\"psychology\",\"business\",\"philosophy\",\"history\"]\n",
        "- \"keywords\": a list of exactly 3 strings\n",
        "\n",
        "This is an output example:\n",
        "{{\n",
        "  \"category\": \"psychology\",\n",
        "  \"keywords\": [\"informed consent\", \"assent\", \"child counseling\"]\n",
        "}}\n",
        "\n",
        "[Question]\n",
        "{question}\n",
        "\"\"\".strip()\n",
        "\n",
        "category_prompt = ChatPromptTemplate.from_template(category_prompt_template)"
      ],
      "metadata": {
        "id": "nzr6RkbISTas"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def classify_and_extract_keywords(question_text: str):\n",
        "#     messages = category_prompt.format_messages(question=question_text)\n",
        "#     #ë©”ì‹œì§€ë¥¼ upstage apië¡œ ë³´ë‚´ì„œ llmì˜ ì‘ë‹µì„ respì— ë„£\n",
        "#     resp = llm_classifier.invoke(messages)\n",
        "#     raw = resp.content.strip()\n",
        "\n",
        "#     data = json.loads(raw)\n",
        "\n",
        "#     category = data[\"category\"].strip().lower()\n",
        "#     keywords = [k.strip() for k in data.get(\"keywords\", [])][:3]\n",
        "\n",
        "#     return category, keywords"
      ],
      "metadata": {
        "id": "2pAwKHXMQx07"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_and_extract_keywords(question_text: str):\n",
        "    messages = category_prompt.format_messages(question=question_text)\n",
        "    resp = llm_classifier.invoke(messages)\n",
        "    raw = resp.content.strip()\n",
        "\n",
        "    # ğŸ” ë””ë²„ê¹…ìš©: ì²˜ìŒì—” í•œë‘ ë²ˆ ì°ì–´ì„œ ì–´ë–¤ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•´ë´\n",
        "    # print(\"RAW CATEGORY OUTPUT:\", repr(raw))\n",
        "\n",
        "    if not raw:\n",
        "        # ì™„ì „ ë¹ˆ ì‘ë‹µì´ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì¶© ì²˜ë¦¬ (ì£½ì§€ ì•Šê²Œ)\n",
        "        print(\"[WARN] Empty LLM output for category, fallback to 'history'\")\n",
        "        return \"history\", [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "    # 1ì°¨ ì‹œë„: ì „ì²´ë¥¼ JSONìœ¼ë¡œ í•´ì„\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        # 2ì°¨ ì‹œë„: ë¬¸ìì—´ ì•ˆì—ì„œ {...} êµ¬ê°„ë§Œ ì˜ë¼ì„œ í•´ì„\n",
        "        start = raw.find(\"{\")\n",
        "        end = raw.rfind(\"}\")\n",
        "        if start == -1 or end == -1:\n",
        "            print(\"[WARN] No JSON object found in output, fallback to 'history'\")\n",
        "            return \"history\", [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "        json_str = raw[start:end+1]\n",
        "        try:\n",
        "            data = json.loads(json_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"[WARN] JSON parse failed again:\", e)\n",
        "            return \"history\", [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "    category = data.get(\"category\", \"history\").strip().lower()\n",
        "    keywords = data.get(\"keywords\", [])\n",
        "    keywords = [str(k).strip() for k in keywords][:3]\n",
        "\n",
        "    if not keywords:\n",
        "        keywords = [\"history\", \"war\", \"europe\"]\n",
        "\n",
        "    return category, keywords"
      ],
      "metadata": {
        "id": "93dJK7O09Jar"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wiki_context(keywords, max_chars_per_page=1200):\n",
        "    snippets = []\n",
        "    for kw in keywords:\n",
        "        try:\n",
        "            page = wiki.page(kw)\n",
        "            if not page.exists():\n",
        "                print(f\"[WARN] page for '{kw}' does not exist\")\n",
        "                continue\n",
        "            text = page.text[:max_chars_per_page]\n",
        "            snippets.append(f\"[Wikipedia: {page.title}]\\n{text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] wikipediaapi fetch failed for '{kw}': {e}\")\n",
        "            continue\n",
        "\n",
        "    return \"\\n\\n\".join(snippets)"
      ],
      "metadata": {
        "id": "e_n3aEbhUxxX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_K_TEXTBOOK = 5\n",
        "\n",
        "def build_context_for_solver(question_text: str, category: str, keywords):\n",
        "    # 1) ì¹´í…Œê³ ë¦¬ë³„ textbook RAG\n",
        "    vs = vectorstore[category]\n",
        "    docs = vs.similarity_search(question_text, k=TOP_K_TEXTBOOK)\n",
        "    textbook_context = \"\\n\\n\".join(\n",
        "        f\"[Textbook Doc {i+1}] {d.page_content}\" for i, d in enumerate(docs)\n",
        "    )\n",
        "\n",
        "    # 2) Wikipedia context\n",
        "    wiki_context = fetch_wiki_context(keywords)\n",
        "\n",
        "    full_context = f\"\"\"\\\n",
        "=== TEXTBOOK CONTEXT ({category}) ===\n",
        "{textbook_context}\n",
        "\n",
        "=== WIKIPEDIA CONTEXT (keywords: {', '.join(keywords)}) ===\n",
        "{wiki_context}\n",
        "\"\"\"\n",
        "    return full_context"
      ],
      "metadata": {
        "id": "XlGDRESCR7Xu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd LLM**:\n",
        "\n",
        "- return CoT\n",
        "- return ìµœì¢… ì •ë‹µ"
      ],
      "metadata": {
        "id": "WLuAVR6GVAwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOLVER_PROMPT_TMPL = \"\"\"\n",
        "You are an expert exam solver.\n",
        "\n",
        "Use the following context (textbook + Wikipedia) to answer the\n",
        "multiple-choice question. Carefully reason step by step, comparing each option\n",
        "with the evidence.\n",
        "\n",
        "At the end, output the final answer in the format:\n",
        "\n",
        "Final Answer: X\n",
        "\n",
        "where X is a single capital letter (A, B, C, D, ...).\n",
        "\n",
        "If the context is incomplete, choose the MOST reasonable answer but do not\n",
        "contradict the given evidence.\n",
        "\n",
        "[CONTEXT]\n",
        "{context}\n",
        "\n",
        "[QUESTION]\n",
        "{question}\n",
        "\n",
        "Now reason step by step, then give the final answer.\n",
        "\"\"\".strip()\n",
        "\n",
        "solver_prompt = ChatPromptTemplate.from_template(SOLVER_PROMPT_TMPL)\n"
      ],
      "metadata": {
        "id": "vGBhIrr7U7ex"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_mmlu_with_rag_and_wiki(question_text: str):\n",
        "    # 1) 1ë‹¨ê³„ LLM: ì¹´í…Œê³ ë¦¬ + í‚¤ì›Œë“œ\n",
        "    category, keywords = classify_and_extract_keywords(question_text)\n",
        "    if category not in vectorstore:\n",
        "        print(f\"[WARN] Unknown category '{category}', fallback to 'business'\")\n",
        "        category = \"business\"\n",
        "\n",
        "    # 2) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (FAISS + Wikipedia)\n",
        "    context = build_context_for_solver(question_text, category, keywords)\n",
        "\n",
        "    # 3) 2ë‹¨ê³„ LLM: CoTë¡œ ì •ë‹µ ë„ì¶œ\n",
        "    messages = solver_prompt.format_messages(\n",
        "        context=context,\n",
        "        question=question_text,\n",
        "    )\n",
        "    resp = llm_solver.invoke(messages)\n",
        "    raw = resp.content.strip()\n",
        "\n",
        "    # 4) \"Final Answer: X\"ì—ì„œ X ì¶”ì¶œ\n",
        "    final_letter = None\n",
        "    for line in raw.splitlines()[::-1]:  # ì•„ë˜ìª½ë¶€í„° ê²€ìƒ‰\n",
        "        line = line.strip()\n",
        "        if line.upper().startswith(\"FINAL ANSWER\"):\n",
        "            # ì˜ˆ: \"Final Answer: D\"\n",
        "            parts = line.split(\":\")\n",
        "            if len(parts) >= 2:\n",
        "                cand = parts[1].strip()\n",
        "                if cand and \"A\" <= cand[0] <= \"Z\":\n",
        "                    final_letter = cand[0]\n",
        "                    break\n",
        "\n",
        "    # ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´, ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ëŒ€ë¬¸ì í•œ ê¸€ì ê²€ìƒ‰\n",
        "    if final_letter is None:\n",
        "        for ch in raw:\n",
        "            if \"A\" <= ch <= \"Z\":\n",
        "                final_letter = ch\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"category\": category,\n",
        "        \"keywords\": keywords,\n",
        "        \"raw_reasoning\": raw,       # ë””ë²„ê¹…/ë¶„ì„ìš©\n",
        "        \"final_answer\": final_letter,\n",
        "    }"
      ],
      "metadata": {
        "id": "aynzekilVXjc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**testset ì‹¤í–‰ ì˜ˆì‹œ**:\n",
        "- baseline.csv"
      ],
      "metadata": {
        "id": "VyGvmOy6VdUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"testset.csv\")[26:]  # ì§ˆë¬¸, ì„ íƒì§€ ë‹¤ í•©ì³ì§„ ì»¬ëŸ¼ ê°€ì •\n",
        "QUESTION_COL = \"prompts\"\n",
        "\n",
        "results = []\n",
        "for i, row in df.iterrows():\n",
        "    q = row[QUESTION_COL]\n",
        "    print(f\"[{i}] Solving: {q[:60]}...\")\n",
        "    out = solve_mmlu_with_rag_and_wiki(q)\n",
        "    results.append(out[\"final_answer\"])\n",
        "    df.loc[i, \"pred_category\"] = out[\"category\"]\n",
        "    df.loc[i, \"kw1\"] = out[\"keywords\"][0] if len(out[\"keywords\"]) > 0 else \"\"\n",
        "    df.loc[i, \"kw2\"] = out[\"keywords\"][1] if len(out[\"keywords\"]) > 1 else \"\"\n",
        "    df.loc[i, \"kw3\"] = out[\"keywords\"][2] if len(out[\"keywords\"]) > 2 else \"\"\n",
        "    df.loc[i, \"rag_cot_answer\"] = out[\"final_answer\"]\n",
        "    df.loc[i, \"cot_full\"] = out[\"raw_reasoning\"]\n",
        "\n",
        "df.to_csv(\"baseline-pro.csv\", index=False)\n",
        "print(\"ì €ì¥ ì™„ë£Œ:baseline-pro.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2PG9zsVVbEQ",
        "outputId": "d0d0aef9-dc79-47fc-94f2-398ab0a61d05"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26] Solving: QUESTION27) A man is at home in his apartment, alone, late a...\n",
            "[27] Solving: QUESTION28) What do Homo sapiens and Australopithecus afaren...\n",
            "[28] Solving: QUESTION29)This question refers to the following information...\n",
            "[WARN] page for 'Tang relations' does not exist\n",
            "[WARN] page for 'frontier peoples' does not exist\n",
            "[29] Solving: QUESTION30)Homo erectus differed from Homo habilis in which ...\n",
            "[30] Solving: QUESTION31)During the manic phase of a bipolar disorder, ind...\n",
            "[WARN] page for 'manic phase' does not exist\n",
            "[WARN] page for 'high self-esteem' does not exist\n",
            "[31] Solving: QUESTION32) This question refers to the following informatio...\n",
            "[32] Solving: QUESTION33) You receive a phone call from Hermann H., age 28...\n",
            "[WARN] page for 'ethical psychologist' does not exist\n",
            "[33] Solving: QUESTION34) During the second stage of Kohlbergâ€™s preconvent...\n",
            "[WARN] page for 'Kohlbergâ€™s theory' does not exist\n",
            "[WARN] page for 'preconventional level' does not exist\n",
            "[34] Solving: QUESTION35)  In satisfying Kant's Humanity formulation of th...\n",
            "[WARN] page for 'Kant's categorical imperative' does not exist\n",
            "[WARN] page for 'Humanity formulation' does not exist\n",
            "[WARN] page for 'morally permissible ends' does not exist\n",
            "[35] Solving: QUESTION36) Aristotle says  that what makes things be what t...\n",
            "[36] Solving: QUESTION37) The ________ School of jurisprudence believes th...\n",
            "[WARN] page for 'social traditions' does not exist\n",
            "[37] Solving: QUESTION38) A woman was standing in the aisle of a subway ca...\n",
            "[38] Solving: QUESTION39) A defendant met her friend at the electronics st...\n",
            "[39] Solving: QUESTION40)____________ refers to a strategic process involv...\n",
            "[WARN] page for 'stakeholder assessment' does not exist\n",
            "[40] Solving: QUESTION41)This question refers to the following information...\n",
            "[41] Solving: QUESTION42) Is the recognition of foreign judgments subject ...\n",
            "[WARN] page for 'doctrine of dualism' does not exist\n",
            "[WARN] page for 'bilateral treaties' does not exist\n",
            "[42] Solving: QUESTION43) Some contemporary intelligence researchers like ...\n",
            "[43] Solving: QUESTION44) BobGafneyand Susan Medina invested $40,000 and $...\n",
            "[WARN] page for 'net income allocation' does not exist\n",
            "[WARN] page for 'partnership investment' does not exist\n",
            "[WARN] page for 'interest on capital' does not exist\n",
            "[44] Solving: QUESTION45) One objection to Singerâ€™s theory that he conside...\n",
            "[WARN] page for 'Singerâ€™s theory' does not exist\n",
            "[45] Solving: QUESTION46) In 1797, John Frere made a discovery that he des...\n",
            "[46] Solving: QUESTION47) Pick the correct description of the following te...\n",
            "[47] Solving: QUESTION48) Which of the following describes a key change in...\n",
            "[48] Solving: QUESTION49) Delia was accepted to both Harvard University an...\n",
            "[WARN] page for 'approach-approach' does not exist\n",
            "[WARN] page for 'cognitive-dissonance' does not exist\n",
            "[49] Solving: QUESTION50) Which is the least accurate description of legal...\n",
            "[WARN] page for 'morality and law' does not exist\n",
            "[WARN] page for 'closed logical system' does not exist\n",
            "ì €ì¥ ì™„ë£Œ:baseline-pro.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì •í™•ë„**"
      ],
      "metadata": {
        "id": "R3J993zAevoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1. íŒŒì¼ ê²½ë¡œ\n",
        "GT_PATH = \"testset.csv\"          # ì •ë‹µ íŒŒì¼\n",
        "PRED_PATH = \"baseline-pro.csv\"       # ì¶œë ¥ íŒŒì¼\n",
        "\n",
        "# 2. ì»¬ëŸ¼ ì´ë¦„\n",
        "GT_COL = \"answers\"               # testset.csvì—ì„œ ì •ë‹µ ì»¬ëŸ¼\n",
        "PRED_COL = \"rag_cot_answer\"      # baseline.csvì—ì„œ ì˜ˆì¸¡ ì»¬ëŸ¼\n",
        "\n",
        "# 3. csv ë¡œë“œ\n",
        "gt_df = pd.read_csv(GT_PATH)[26:]      # 26ë²ˆ ë¬¸ì œë¶€í„°ë¼ê³  ê°€ì •\n",
        "pred_df = pd.read_csv(PRED_PATH)\n",
        "\n",
        "# 4. Series ì¶”ì¶œ\n",
        "gt = gt_df[GT_COL].astype(str)\n",
        "pred = pred_df[PRED_COL].astype(str)\n",
        "\n",
        "# 5. ì •ê·œí™” í•¨ìˆ˜\n",
        "def normalize_choice(x: str) -> str:\n",
        "    x = x.strip().upper()\n",
        "    for ch in x:\n",
        "        if \"A\" <= ch <= \"Z\":\n",
        "            return ch\n",
        "    return x\n",
        "\n",
        "# ì¸ë±ìŠ¤ ë¦¬ì…‹ì´ í¬ì¸íŠ¸\n",
        "gt_norm = gt.apply(normalize_choice).reset_index(drop=True)\n",
        "pred_norm = pred.apply(normalize_choice).reset_index(drop=True)\n",
        "\n",
        "print(len(gt_norm), len(pred_norm))  # ë‘˜ ë‹¤ 25 ë‚˜ì˜¤ëŠ”ì§€ ì²´í¬ í•œë²ˆ í•´ë³´ê³ \n",
        "\n",
        "# 6. ì •í™•ë„ ê³„ì‚°\n",
        "correct = (gt_norm == pred_norm)\n",
        "accuracy = correct.mean()\n",
        "\n",
        "print(f\"ì´ ë¬¸ì œ ìˆ˜: {len(gt_norm)}\")\n",
        "print(f\"ì •ë‹µ ê°œìˆ˜: {correct.sum()}\")\n",
        "print(f\"ì •í™•ë„: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhvOdmVFVtvI",
        "outputId": "763efc1f-e758-4f3b-ab5d-91d71ada333b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24 24\n",
            "ì´ ë¬¸ì œ ìˆ˜: 24\n",
            "ì •ë‹µ ê°œìˆ˜: 10\n",
            "ì •í™•ë„: 41.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TIwiC6e0e44n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}