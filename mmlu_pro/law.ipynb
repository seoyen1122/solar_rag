{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyen1122/solar_rag/blob/main/mmlu_pro/law.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-6OW5UE6tali"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qU python-dotenv PyPDF2 langchain langchain-community langchain-core langchain-text-splitters langchain_upstage oracledb python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LEsWxsJjiTqZ"
      },
      "outputs": [],
      "source": [
        "! pip install -q openai langchain tiktoken faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TCh-CfFpya_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4745032-6e05-474d-8aef-00d8e3147c8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2mTZZU4Ps3nU"
      },
      "outputs": [],
      "source": [
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ookX6xz1tCDj"
      },
      "outputs": [],
      "source": [
        "UPSTAGE_API_KEY = \"up_g7T2cQoLKZH6Oi2n4MHOW706XAdSs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hB2GFt5Stg1a"
      },
      "outputs": [],
      "source": [
        "upstage_embeddings = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=\"embedding-passage\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/2025_2/nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6OQFdyOy0jb",
        "outputId": "9c46c087-6686-4325-f1a2-ac891bc473b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/2025_2/nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avXEWwZbGM3f"
      },
      "source": [
        "# Cornell.edu\n",
        "**알파벳 전체를 돌면서 URL 리스트 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZA5T-y8yuBCH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import string\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "u3tKaY5gGMEJ"
      },
      "outputs": [],
      "source": [
        "base = \"https://www.law.cornell.edu\"\n",
        "index_template = base + \"/wex/all/{letter}\"\n",
        "\n",
        "header = {\"User-Agent\": \"law/0.1 (for project; contact:seoyen1122@gmail.com)\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "L9jsf1-EGlpC"
      },
      "outputs": [],
      "source": [
        "def collect_links(letters=None):\n",
        "  if letters is None:\n",
        "    letters = list(string.ascii_lowercase)\n",
        "\n",
        "  seen = set()\n",
        "  entries = []\n",
        "\n",
        "  for letter in letters:\n",
        "    index_url = index_template.format(letter = letter)\n",
        "    resp = requests.get(index_url, headers = header) #requests- 웹사이트에 요청보내고응답받기, index_url 주소로 httpget 요청 보냄, header = 나는 이런 이름의 봇이고 프로젝트 목적으로 요청 보낸다는 정보 전달용, return: request.Response객체, 이 페이지 정보, resp.status_code, resp.text, resp.content\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\") #다운받은 html문자열을 파싱해서 태그들을 쉽게 찾을 수 있는 객체로 바꾸는 코드.\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True): #이 페이지 안에서 href 속성이 있는 모든 <a> 태그를 찾아라 → 그 안에 들어있는 링크 주소(href)를 하나씩 꺼내자\n",
        "      href = a[\"href\"]\n",
        "\n",
        "      if not href.startswith(\"/wex/\"):\n",
        "        continue\n",
        "      if \"/wex/all\" in href:\n",
        "        continue\n",
        "\n",
        "      full_url = urljoin(base, href)\n",
        "      if full_url in seen:\n",
        "        continue\n",
        "\n",
        "      term = a.get_text(strip=True)\n",
        "      entries.append({\"term\": term, \"url\":full_url})\n",
        "      seen.add(full_url)\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "  print(\"total entries:\", len(entries))\n",
        "  return entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LEIf_pGKJ9c"
      },
      "source": [
        "**각 항목 페이지에서 본문 텍스트 추출**\n",
        "\n",
        "상단 네비게이션\n",
        "\n",
        "h1 제목(용어)\n",
        "\n",
        "그 아래 여러 단락/리스트가 정의/설명\n",
        "\n",
        "맨 아래에 wex toolbox, terms of use같은 공통 푸터\n",
        "\n",
        "main 안에 있는 p, ul, ol 태그 텍스트를 합치는 식으로 본문만 자르기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "j93NfgVAJ6BU"
      },
      "outputs": [],
      "source": [
        "def fetch_article(url):\n",
        "  resp = requests.get(url, headers = header)\n",
        "  resp.raise_for_status()\n",
        "  soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "  main = soup.find(\"main\") or soup\n",
        "\n",
        "  #제목(h1/h2)\n",
        "  title_tag = main.find([\"h1\", \"h2\"])\n",
        "  title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "\n",
        "  #푸터/툴박스 부분 대충 제거 (string 에 \"Wex Toolbox\"가 들어가면 그 뒤는 날려버기)\n",
        "  toolbox = main.find(string=lambda s: isinstance(s, str) and \"Wex Toolbox\" in s)\n",
        "  if toolbox:\n",
        "    parent = toolbox.parent\n",
        "    sib = parent.next_sibling\n",
        "    while sib is not None:\n",
        "      next_sib = sib.next_sibling\n",
        "      try:\n",
        "        sib.decompose()\n",
        "      except Exception:\n",
        "        pass\n",
        "      sib = next_sib\n",
        "\n",
        "  chunks = []\n",
        "  current_title = title or \"Definition\"\n",
        "  current_texts = []\n",
        "\n",
        "  for element in main.find_all([\"h2\", \"p\", \"li\"], recursive=True):\n",
        "    if element.name == \"h2\":\n",
        "      if current_texts:\n",
        "        text = \"\\n\\n\".join(current_texts).strip()\n",
        "        if text:\n",
        "          chunks.append({\n",
        "              \"section_title\": current_title,\n",
        "              \"text\": text\n",
        "          })\n",
        "        current_texts = []\n",
        "      current_title = element.get_text(\" \", strip=True) or title\n",
        "\n",
        "    elif element.name in [\"p\", \"li\"]:\n",
        "      txt = element.get_text(\" \", strip=True)\n",
        "      if txt:\n",
        "        current_texts.append(txt)\n",
        "\n",
        "  if current_texts:\n",
        "    text = \"\\n\\n\".join(current_texts).strip()\n",
        "    if text:\n",
        "      chunks.append({\n",
        "          \"section_title\": current_title,\n",
        "          \"text\": text\n",
        "      })\n",
        "\n",
        "\n",
        "  # 혹시 아무 청크도 못 만들었으면, 전체 p/li를 한 덩어리로라도 넣기 (optional)\n",
        "  if not chunks:\n",
        "      texts = []\n",
        "      for element in main.find_all([\"p\", \"li\"]):\n",
        "          txt = element.get_text(\" \", strip=True)\n",
        "          if txt:\n",
        "              texts.append(txt)\n",
        "      if texts:\n",
        "          chunks.append({\n",
        "              \"section_title\": title or \"Definition\",\n",
        "              \"text\": \"\\n\\n\".join(texts)\n",
        "          })\n",
        "\n",
        "    # 최종 메타데이터 dict 반환\n",
        "  meta = {\n",
        "      \"source_url\": url,\n",
        "      \"title\": title,\n",
        "      \"chunk_list\": chunks,\n",
        "  }\n",
        "  return meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysedD44YL_TT"
      },
      "source": [
        "**전체 크롤링 + jsonl로 저장**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gJd8xDAXL7xU"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VQIYk2lwMF3r"
      },
      "outputs": [],
      "source": [
        "def crawl(output_path=\"wex_raw.jsonl\", letters=None):\n",
        "  entries = collect_links(letters)\n",
        "\n",
        "  with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, entry in enumerate(entries):\n",
        "      url = entry[\"url\"]\n",
        "      meta = fetch_article(url)\n",
        "\n",
        "      if meta.get(\"chunk_list\"):\n",
        "        f.write(json.dumps(meta, ensure_ascii=False) + \"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "      time.sleep(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItivvmZkOb94"
      },
      "source": [
        "**청킹 + 임베딩 + FAISS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5RcEcVe6PfTj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = UPSTAGE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "V8HnJ0Y1OaYs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_upstage import UpstageEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RjXh0WkmPt58"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_chars=800, overlap_chars=200):\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  n = len(text)\n",
        "  while start < n:\n",
        "    end = min(start + max_chars, n)\n",
        "    chunk = text[start:end]\n",
        "    chunks.append(chunk)\n",
        "    if end == n:\n",
        "      break\n",
        "    start = end - overlap_chars\n",
        "  return chunks\n",
        "\n",
        "def load_chunks(jsonl_path, max_chars=None, overlap_chars=200):\n",
        "  chunks = []\n",
        "  metadata = []\n",
        "\n",
        "  with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      rec = json.loads(line)\n",
        "      source_url = rec[\"source_url\"]\n",
        "      title = rec[\"title\"]\n",
        "      chunk_list = rec.get(\"chunk_list\", [])\n",
        "\n",
        "      for sec_idx, c in enumerate(chunk_list):\n",
        "          section_title = c.get(\"section_title\", title)\n",
        "          text = c.get(\"text\", \"\")\n",
        "          if not text.strip():\n",
        "              continue\n",
        "\n",
        "          if max_chars is None:\n",
        "            chunks.append(text)\n",
        "            metadata.append({\n",
        "              \"source_url\": source_url,\n",
        "              \"title\": title,\n",
        "              \"section_title\": section_title,\n",
        "              \"section_index\": sec_idx,\n",
        "              \"subchunk_index\": 0,\n",
        "            })\n",
        "          else:\n",
        "              for sub_idx, ch in enumerate(chunk_text(text, max_chars, overlap_chars)):\n",
        "                chunks.append(ch)\n",
        "                metadata.append({\n",
        "                  \"source_url\": source_url,\n",
        "                  \"title\": title,\n",
        "                  \"section_title\": section_title,\n",
        "                  \"section_index\": sec_idx,\n",
        "                  \"subchunk_index\": sub_idx,\n",
        "                })\n",
        "    return chunks, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "pZ8tO6iPQsEt"
      },
      "outputs": [],
      "source": [
        "def build_faiss_index(\n",
        "    jsonl_path = \"wex_structured.jsonl\",\n",
        "    index_path = \"wex_faiss.index\",\n",
        "    meta_path = \"wex_metadata.jsonl\",\n",
        "):\n",
        "  chunks, metadata = load_chunks(jsonl_path, max_chars=None)\n",
        "  print(\"num_chunks:\", len(chunks))\n",
        "\n",
        "  embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "\n",
        "  vecs = embeddings.embed_documents(chunks)\n",
        "  emb = np.array(vecs, dtype=np.float32)\n",
        "  print(\"emb shape\", emb.shape)\n",
        "\n",
        "  dim = emb.shape[1]\n",
        "\n",
        "  faiss.normalize_L2(emb)\n",
        "  index = faiss.IndexFlatIP(dim)\n",
        "  index.add(emb)\n",
        "\n",
        "  faiss.write_index(index, index_path)\n",
        "\n",
        "  with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for m in metadata:\n",
        "      f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "  print(\"index saved to\", index_path)\n",
        "  print(\"metadata saved to\", meta_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FVJzMml0Bk",
        "outputId": "36c443a4-abde-42d0-9122-bda98f0d1a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total entries: 5456\n",
            "num_chunks: 5544\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4000 tokens, but your request contains 4403 tokens. Please reduce the length of your input text or select only the most relevant portions to include in your request. For information on token counting methods and model-specific limits, please refer to our API reference documentation (https://console.upstage.ai/api/embeddings)\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_body'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2022654103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wex_structured_all.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m build_faiss_index(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mjsonl_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wex_structured_all.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindex_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wex_faiss_all.index\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmeta_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wex_metadata_all.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-291168466.py\u001b[0m in \u001b[0;36mbuild_faiss_index\u001b[0;34m(jsonl_path, index_path, meta_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUpstageEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"solar-embedding-1-large-passage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emb shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_upstage/embeddings.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4000 tokens, but your request contains 4403 tokens. Please reduce the length of your input text or select only the most relevant portions to include in your request. For information on token counting methods and model-specific limits, please refer to our API reference documentation (https://console.upstage.ai/api/embeddings)\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_body'}}"
          ]
        }
      ],
      "source": [
        "crawl(\"wex_structured_all.jsonl\", letters=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Y2y5OJiYoz9X"
      },
      "outputs": [],
      "source": [
        "from langchain_upstage import UpstageEmbeddings\n",
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "\n",
        "def build_faiss_index(\n",
        "    jsonl_path=\"wex_structured_all.jsonl\",\n",
        "    index_path=\"wex_faiss_all.index\",\n",
        "    meta_path=\"wex_metadata_all.jsonl\",\n",
        "):\n",
        "    # 1) 청크 + 메타데이터 로드\n",
        "    chunks, metadata = load_chunks(jsonl_path, max_chars=800)\n",
        "    print(\"num_chunks:\", len(chunks))\n",
        "\n",
        "    # 2) Upstage 임베딩 모델 (★ suffix 빼고 base 이름만 사용)\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        "\n",
        "    # 3) 배치로 나눠서 embed_documents 호출 (예: 256개씩)\n",
        "    all_vecs = []\n",
        "    batch_size = 256\n",
        "\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i : i + batch_size]\n",
        "        # 혹시라도 공백 문자열이 섞여 있으면 제거 (안전)\n",
        "        batch = [c for c in batch if c.strip()]\n",
        "        if not batch:\n",
        "            continue\n",
        "\n",
        "        vecs = embeddings.embed_documents(batch)\n",
        "        all_vecs.extend(vecs)\n",
        "        print(f\"embedded {i + len(batch)} / {len(chunks)}\")\n",
        "\n",
        "    emb = np.array(all_vecs, dtype=np.float32)\n",
        "    print(\"emb shape\", emb.shape)\n",
        "\n",
        "    dim = emb.shape[1]\n",
        "    faiss.normalize_L2(emb)\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(emb)\n",
        "\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for m in metadata:\n",
        "            f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(\"index saved to\", index_path)\n",
        "    print(\"metadata saved to\", meta_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "build_faiss_index(\n",
        "    jsonl_path=\"wex_structured_all.jsonl\",\n",
        "    index_path=\"wex_faiss_all.index\",\n",
        "    meta_path=\"wex_metadata_all.jsonl\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOeJI974vkWr",
        "outputId": "a8772ded-1769-486d-ecf1-eca0c486ddbc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_chunks: 14385\n",
            "embedded 256 / 14385\n",
            "embedded 512 / 14385\n",
            "embedded 768 / 14385\n",
            "embedded 1024 / 14385\n",
            "embedded 1280 / 14385\n",
            "embedded 1536 / 14385\n",
            "embedded 1792 / 14385\n",
            "embedded 2048 / 14385\n",
            "embedded 2304 / 14385\n",
            "embedded 2560 / 14385\n",
            "embedded 2816 / 14385\n",
            "embedded 3072 / 14385\n",
            "embedded 3328 / 14385\n",
            "embedded 3584 / 14385\n",
            "embedded 3840 / 14385\n",
            "embedded 4096 / 14385\n",
            "embedded 4352 / 14385\n",
            "embedded 4608 / 14385\n",
            "embedded 4864 / 14385\n",
            "embedded 5120 / 14385\n",
            "embedded 5376 / 14385\n",
            "embedded 5632 / 14385\n",
            "embedded 5888 / 14385\n",
            "embedded 6144 / 14385\n",
            "embedded 6400 / 14385\n",
            "embedded 6656 / 14385\n",
            "embedded 6912 / 14385\n",
            "embedded 7168 / 14385\n",
            "embedded 7424 / 14385\n",
            "embedded 7680 / 14385\n",
            "embedded 7936 / 14385\n",
            "embedded 8192 / 14385\n",
            "embedded 8448 / 14385\n",
            "embedded 8704 / 14385\n",
            "embedded 8960 / 14385\n",
            "embedded 9216 / 14385\n",
            "embedded 9472 / 14385\n",
            "embedded 9728 / 14385\n",
            "embedded 9984 / 14385\n",
            "embedded 10240 / 14385\n",
            "embedded 10496 / 14385\n",
            "embedded 10752 / 14385\n",
            "embedded 11008 / 14385\n",
            "embedded 11264 / 14385\n",
            "embedded 11520 / 14385\n",
            "embedded 11776 / 14385\n",
            "embedded 12032 / 14385\n",
            "embedded 12288 / 14385\n",
            "embedded 12544 / 14385\n",
            "embedded 12800 / 14385\n",
            "embedded 13056 / 14385\n",
            "embedded 13312 / 14385\n",
            "embedded 13568 / 14385\n",
            "embedded 13824 / 14385\n",
            "embedded 14080 / 14385\n",
            "embedded 14336 / 14385\n",
            "embedded 14385 / 14385\n",
            "emb shape (14385, 4096)\n",
            "index saved to wex_faiss_all.index\n",
            "metadata saved to wex_metadata_all.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0ylZnu2-qW4",
        "outputId": "9064161c-d6ef-450b-87b0-aae926830ab5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.4.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "create_faiss_index.py\n",
        "\n",
        "- 'entries.jsonl' (Semantic Chunking된 파일)을 읽어옵니다.\n",
        "- 각 청크(섹션)를 로드합니다.\n",
        "- (안전 장치) 만약 섹션 텍스트가 1000자를 넘으면, 1000자 단위로 더 잘게 자릅니다.\n",
        "- 이 과정에서 'title', 'source_url', 'section_title' 메타데이터를 모두 보존합니다.\n",
        "- BGE-small 임베딩 모델을 사용하여 모든 청크를 임베딩합니다.\n",
        "- 'faiss_index_philosophy'라는 이름의 로컬 FAISS 인덱스로 저장합니다.\n",
        "\"\"\"\n",
        "\n",
        "import jsonlines\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. 설정값\n",
        "# ----------------------------------------------------\n",
        "JSONL_FILE = \"wex_structured_all.jsonl\"             # 입력 파일 (스크래핑 결과)\n",
        "INDEX_NAME = \"faiss_index_law\"    # 저장할 FAISS 인덱스 이름\n",
        "\n",
        "# \"Safety Net\" 청킹 설정 (H2 섹션이 너무 클 경우 대비)\n",
        "CHUNK_SIZE = 1000   # 청크 최대 글자 수\n",
        "CHUNK_OVERLAP = 100 # 청크 겹침\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. 임베딩 모델 로드\n",
        "# ----------------------------------------------------\n",
        "try:\n",
        "    upstage_embeddings\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model. Do you have a GPU runtime? {e}\")\n",
        "    # (GPU 런타임이 아닐 경우 CPU로 fallback)\n",
        "    # model_kwargs = {'device': 'cpu'}\n",
        "    # embedding_model = HuggingFaceEmbeddings(\n",
        "    #     model_name=model_name,\n",
        "    #     model_kwargs=model_kwargs,\n",
        "    #     encode_kwargs=encode_kwargs\n",
        "    # )\n",
        "print(\"Embedding model loaded.\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. JSONL 로드 및 '안전 장치' 청킹\n",
        "# ----------------------------------------------------\n",
        "print(f\"Loading '{JSONL_FILE}' and applying safety net chunking...\")\n",
        "\n",
        "# '안전 장치' (H2 섹션이 너무 클 경우를 대비한) 텍스트 분할기\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "all_final_chunks = [] # 최종적으로 FAISS에 들어갈 Document 객체 리스트\n",
        "\n",
        "try:\n",
        "    with jsonlines.open(JSONL_FILE, 'r') as reader:\n",
        "        for entry in reader:\n",
        "            # (1) 기본 메타데이터 (페이지 레벨)\n",
        "            base_metadata = {\n",
        "                \"source\": entry.get(\"source_url\", \"N/A\"),\n",
        "                \"title\": entry.get(\"title\", \"N/A\"),\n",
        "            }\n",
        "\n",
        "            # (2) Semantic Chunking된 'chunk_list' 순회\n",
        "            for chunk in entry.get(\"chunk_list\", []):\n",
        "                section_text = chunk.get(\"text\")\n",
        "                section_title = chunk.get(\"section_title\", \"N/A\")\n",
        "\n",
        "                if not section_text:\n",
        "                    continue\n",
        "\n",
        "                # (3) H2 섹션 텍스트가 CHUNK_SIZE(1000자)를 넘을 경우,\n",
        "                #     text_splitter가 이 텍스트를 더 작은 '미니 청크'로 자름\n",
        "                split_texts = text_splitter.split_text(section_text)\n",
        "\n",
        "                # (4) 이 '미니 청크'들을 Document 객체로 변환\n",
        "                for text_piece in split_texts:\n",
        "                    # 메타데이터에 'section' 정보를 추가\n",
        "                    final_metadata = base_metadata.copy()\n",
        "                    final_metadata[\"section\"] = section_title\n",
        "\n",
        "                    new_doc = Document(page_content=text_piece, metadata=final_metadata)\n",
        "                    all_final_chunks.append(new_doc)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{JSONL_FILE}' not found. Please run the scraping script first.\")\n",
        "    sys.exit()\n",
        "\n",
        "print(f\"Total 'mini-chunks' to be indexed: {len(all_final_chunks)}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. FAISS 임베딩 및 저장\n",
        "# ----------------------------------------------------\n",
        "if all_final_chunks:\n",
        "    print(\"Starting FAISS index creation... (This may take a long time)\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # FAISS.from_documents()를 사용하면\n",
        "    # 텍스트 청크는 임베딩되고, 메타데이터는 그대로 벡터 스토어에 저장됩니다.\n",
        "    db_psychology = FAISS.from_documents(all_final_chunks, upstage_embeddings)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"FAISS index created successfully in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 생성된 인덱스를 파일로 저장\n",
        "    db_psychology.save_local(INDEX_NAME)\n",
        "\n",
        "    print(f\"FAISS index saved to folder: '{INDEX_NAME}'\")\n",
        "else:\n",
        "    print(\"No chunks were created. FAISS index not built.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXEibfqWwCTB",
        "outputId": "2771c524-a0bd-48b5-a3e3-9991a7c42956"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model loaded.\n",
            "Loading 'wex_structured_all.jsonl' and applying safety net chunking...\n",
            "Total 'mini-chunks' to be indexed: 12089\n",
            "Starting FAISS index creation... (This may take a long time)\n",
            "FAISS index created successfully in 1463.01 seconds.\n",
            "FAISS index saved to folder: 'faiss_index_law'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gc5vPCgY_VL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "mount_file_id": "1GpAg6Z8AG45fWZlznCuVYesm5fOYDUIE",
      "authorship_tag": "ABX9TyNwfXIbjW8ZCfFCrpaz/nQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}