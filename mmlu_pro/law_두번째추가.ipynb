{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1gHSQFUx4cz-npNQwH-lvWc5dlW08L-C9",
      "authorship_tag": "ABX9TyMJflFclhfa8RBJtly6s7KB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyen1122/solar_rag/blob/main/mmlu_pro/law_%EB%91%90%EB%B2%88%EC%A7%B8%EC%B6%94%EA%B0%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vvhaEiTF6cP",
        "outputId": "18a42f57-034e-4ba1-c0cd-54a1f4c1766d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20251107 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -qU python-dotenv PyPDF2 langchain langchain-community langchain-core langchain-text-splitters langchain_upstage oracledb python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XArzTzENGADa",
        "outputId": "ab243817-6c5a-4f43-9114-37b0725cfadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "transformers 4.57.1 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tlsmVPPG_QN",
        "outputId": "e8d1f75e-0595-417a-9985-e7130c4f95cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrKZE5xZFzyD"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
        "import pdfplumber\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/2025_2/nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka5-uqJ2G6s2",
        "outputId": "17cc702d-ba4a-4d50-d169-abbd28922c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/2025_2/nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INDEX_NAME = \"faiss_index_law\"\n",
        "HELIX_PDFS = [\n",
        "    (\"helix_torts_mini-outline.pdf\", \"torts\"),\n",
        "    (\"helix_real_property_mini-outline.pdf\", \"real property\"),\n",
        "    (\"helix_evidence_mini-outline.pdf\", \"evidence\"),\n",
        "    (\"helix_criminal_procedure_mini-outline.pdf\", \"criminal procedure\"),\n",
        "    (\"helix_criminal_law_mini-outline.pdf\", \"criminal law\"),\n",
        "    (\"helix_contracts_mini-outline.pdf\", \"contracts\"),\n",
        "    (\"helix_constitutional_law_mini-outline.pdf\", \"constitutional law\"),\n",
        "    (\"helix_civil_procedure_mini-outline.pdf\", \"civil procedure\")\n",
        "]"
      ],
      "metadata": {
        "id": "kzz0pE7CF52r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_JSONL = \"helix_structured_all.jsonl\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 100"
      ],
      "metadata": {
        "id": "YA7MIK4nGZFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UPSTAGE_API_KEY = \"up_g7T2cQoLKZH6Oi2n4MHOW706XAdSs\"\n",
        "embeddings = UpstageEmbeddings(api_key = UPSTAGE_API_KEY, model=\"solar-embedding-1-large\")"
      ],
      "metadata": {
        "id": "Lid484DTGbzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.load_local(\n",
        "    INDEX_NAME,\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True,  # langchain>=0.2.x에서 필요\n",
        ")"
      ],
      "metadata": {
        "id": "hIWfCHiFGj8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")"
      ],
      "metadata": {
        "id": "KhC4x60UG8ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_heading(line: str, subject: str | None = None) -> bool:\n",
        "    \"\"\"Helix 스타일에 맞춘 제목 감지 휴리스틱.\"\"\"\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        return False\n",
        "    # 페이지 번호, 푸터 등 버리기\n",
        "    if re.match(r\"^\\d+\\s+Helix Bar Review\", line):\n",
        "        return False\n",
        "\n",
        "    if \"Helix Bar Review\" in line:\n",
        "        return False\n",
        "\n",
        "    if subject and line.upper() == subject.upper():\n",
        "        return False\n",
        "\n",
        "    if line.startswith(\"Prima Facie Case:\"):\n",
        "        return False\n",
        "\n",
        "    # 완전 대문자 + 숫자/공백/기호 정도면 heading 가능성 높음\n",
        "    if re.match(r\"^[A-Z0-9\\s\\-’'&/]+$\", line) and 5 <= len(line) <= 50:\n",
        "        return True\n",
        "\n",
        "    # 맨 앞 대문자, 나머지 소문자/공백 정도의 짧은 라인 (예: \"Defenses to Intentional Torts\")\n",
        "    if len(line) <= 60 and line[0].isupper() and line.isprintable() and line.count(\".\") == 0:\n",
        "        # 문장 느낌보다는 제목 느낌\n",
        "        return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "m4DSz5iZJLVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lines_from_pdf(pdf_path: str):\n",
        "    lines = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text() or \"\"\n",
        "            # 줄 단위 split\n",
        "            for line in text.splitlines():\n",
        "                line = line.rstrip()\n",
        "                if line:\n",
        "                    lines.append(line)\n",
        "    return lines"
      ],
      "metadata": {
        "id": "VeRA9Hs7JmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_chunk_list_from_lines(lines, subject):\n",
        "    \"\"\"Helix outline 텍스트 라인들을 section_title 기반 chunk_list로 변환.\"\"\"\n",
        "    chunk_list = []\n",
        "\n",
        "    current_title = None\n",
        "    current_text_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if is_heading(line, subject):\n",
        "            # INTRODUCTION 같은 pure 안내 섹션은 건너뛰고 싶으면 여기서 필터\n",
        "            if current_title and current_text_lines:\n",
        "                chunk_list.append({\n",
        "                    \"section_title\": current_title,\n",
        "                    \"text\": \"\\n\".join(current_text_lines).strip()\n",
        "                })\n",
        "            current_title = line.strip()\n",
        "            current_text_lines = []\n",
        "        else:\n",
        "            # 본문 라인\n",
        "            if current_title is None:\n",
        "                # 아직 제목이 안나온 상태면 스킵하거나 \"INTRO\"로 묶을 수 있음\n",
        "                continue\n",
        "            current_text_lines.append(line.strip())\n",
        "\n",
        "    # 마지막 섹션 flush\n",
        "    if current_title and current_text_lines:\n",
        "        chunk_list.append({\n",
        "            \"section_title\": current_title,\n",
        "            \"text\": \"\\n\".join(current_text_lines).strip()\n",
        "        })\n",
        "\n",
        "    return chunk_list"
      ],
      "metadata": {
        "id": "QES3YyDCU8c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with open(OUTPUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
        "        for pdf_path, subject in HELIX_PDFS:\n",
        "            lines = extract_lines_from_pdf(pdf_path)\n",
        "            chunk_list = build_chunk_list_from_lines(lines, subject)\n",
        "\n",
        "            # 제목은 대충 파일명에서 생성\n",
        "            title = f\"Helix {subject.title()} Mini-Outline\"\n",
        "\n",
        "            meta = {\n",
        "                \"source_url\": f\"helix::{pdf_path}\",\n",
        "                \"title\": title,\n",
        "                \"doc_type\": \"helix\",\n",
        "                \"subject\": subject,\n",
        "                \"chunk_list\": chunk_list,\n",
        "            }\n",
        "\n",
        "            f.write(json.dumps(meta, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"[DONE] Written structured Helix jsonl to {OUTPUT_JSONL}\")\n"
      ],
      "metadata": {
        "id": "ofxJYR7vU_o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO8T16pIVN7E",
        "outputId": "d4df5a63-a9fb-420d-b58f-ee7d7d9dea7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] Written structured Helix jsonl to helix_structured_all.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "jsonl to index.faiss & index.pkl"
      ],
      "metadata": {
        "id": "q9Hq7KEwbN4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 네가 이미 쓰고 있던 함수 그대로 사용\n",
        "def chunk_text(text, max_chars=800, overlap_chars=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(start + max_chars, n)\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        if end == n:\n",
        "            break\n",
        "        start = end - overlap_chars\n",
        "    return chunks\n",
        "\n",
        "def load_chunks_multi(jsonl_paths, max_chars=None, overlap_chars=200):\n",
        "    chunks = []\n",
        "    metadata = []\n",
        "\n",
        "    for jsonl_path in jsonl_paths:\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                rec = json.loads(line)\n",
        "                source_url = rec[\"source_url\"]\n",
        "                title = rec[\"title\"]\n",
        "                doc_type = rec.get(\"doc_type\", \"wex\")\n",
        "                subject = rec.get(\"subject\")   # helix용\n",
        "                chunk_list = rec.get(\"chunk_list\", [])\n",
        "\n",
        "                for sec_idx, c in enumerate(chunk_list):\n",
        "                    section_title = c.get(\"section_title\", title)\n",
        "                    text = c.get(\"text\", \"\")\n",
        "                    if not text.strip():\n",
        "                        continue\n",
        "\n",
        "                    base_meta = {\n",
        "                        \"source_url\": source_url,\n",
        "                        \"title\": title,\n",
        "                        \"doc_type\": doc_type,\n",
        "                        \"section_title\": section_title,\n",
        "                        \"section_index\": sec_idx,\n",
        "                    }\n",
        "                    if subject is not None:\n",
        "                        base_meta[\"subject\"] = subject\n",
        "\n",
        "                    if max_chars is None:\n",
        "                        chunks.append(text)\n",
        "                        m = base_meta.copy()\n",
        "                        m[\"subchunk_index\"] = 0\n",
        "                        metadata.append(m)\n",
        "                    else:\n",
        "                        for sub_idx, ch in enumerate(\n",
        "                            chunk_text(text, max_chars, overlap_chars)\n",
        "                        ):\n",
        "                            chunks.append(ch)\n",
        "                            m = base_meta.copy()\n",
        "                            m[\"subchunk_index\"] = sub_idx\n",
        "                            metadata.append(m)\n",
        "\n",
        "    return chunks, metadata\n",
        "\n",
        "# ★ 헬릭스 jsonl만 로드\n",
        "helix_chunks, helix_meta = load_chunks_multi(\n",
        "    [\"helix_structured_all.jsonl\"],\n",
        "    max_chars=800,\n",
        "    overlap_chars=200,\n",
        ")\n",
        "\n",
        "helix_docs = [\n",
        "    Document(page_content=txt, metadata=m)\n",
        "    for txt, m in zip(helix_chunks, helix_meta)\n",
        "]\n",
        "\n",
        "print(\"num helix docs:\", len(helix_docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4QrLZ18VPQw",
        "outputId": "e083e6cc-f9e0-40ae-933f-0d09dc4e0154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num helix docs: 629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "\n",
        "INDEX_NAME = \"faiss_index_law\"  # index.faiss / index.pkl 있는 폴더\n",
        "\n",
        "# 1) 임베딩 & 기존 인덱스 로드\n",
        "embeddings = UpstageEmbeddings(api_key = UPSTAGE_API_KEY, model=\"solar-embedding-1-large\")\n",
        "db = FAISS.load_local(\n",
        "    INDEX_NAME,\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True,  # langchain 0.2+ 이면 필요\n",
        ")\n",
        "print(\"loaded existing index\")\n",
        "\n",
        "# 2) 헬릭스 문서 추가\n",
        "db.add_documents(helix_docs)\n",
        "print(\"added helix docs\")\n",
        "\n",
        "# 3) 다시 저장 (덮어쓰기)\n",
        "db.save_local(INDEX_NAME)\n",
        "print(f\"updated index saved to folder: {INDEX_NAME}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evIIFc6ZbjIP",
        "outputId": "084a8524-9c03-416d-de62-145e278a9343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded existing index\n",
            "added helix docs\n",
            "updated index saved to folder: faiss_index_law\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZBucPfabqpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}